<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/config/settings.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/config/settings.py" />
              <option name="originalContent" value="import os&#10;from dotenv import load_dotenv&#10;&#10;class Settings:&#10;    &quot;&quot;&quot;Application settings loaded from environment variables&quot;&quot;&quot;&#10;&#10;    # Load environment variables&#10;    print(&quot;[CONFIG] Loading environment variables...&quot;)&#10;    load_dotenv()&#10;&#10;    # Azure DevOps settings&#10;    ADO_ORGANIZATION = os.getenv('ADO_ORGANIZATION')&#10;    ADO_PROJECT = os.getenv('ADO_PROJECT')&#10;    ADO_PAT = os.getenv('ADO_PAT')&#10;    ADO_BASE_URL = &quot;https://dev.azure.com&quot;&#10;&#10;    # Work item types&#10;    REQUIREMENT_TYPE = os.getenv('ADO_REQUIREMENT_TYPE', 'Epic')&#10;    USER_STORY_TYPE = os.getenv('ADO_USER_STORY_TYPE', 'User Story')&#10;&#10;    # OpenAI settings&#10;    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')&#10;    OPENAI_MAX_RETRIES = int(os.getenv('OPENAI_MAX_RETRIES', 3))&#10;&#10;    @classmethod&#10;    def validate(cls):&#10;        &quot;&quot;&quot;Validate required settings are present&quot;&quot;&quot;&#10;        print(&quot;[CONFIG] Validating settings...&quot;)&#10;        missing = []&#10;&#10;        # Check Azure DevOps settings&#10;        if not cls.ADO_ORGANIZATION:&#10;            missing.append(&quot;ADO_ORGANIZATION&quot;)&#10;            print(&quot;[ERROR] ADO_ORGANIZATION is not set&quot;)&#10;        else:&#10;            print(f&quot;[CONFIG] ADO_ORGANIZATION: {cls.ADO_ORGANIZATION}&quot;)&#10;&#10;        if not cls.ADO_PROJECT:&#10;            missing.append(&quot;ADO_PROJECT&quot;)&#10;            print(&quot;[ERROR] ADO_PROJECT is not set&quot;)&#10;        else:&#10;            print(f&quot;[CONFIG] ADO_PROJECT: {cls.ADO_PROJECT}&quot;)&#10;&#10;        if not cls.ADO_PAT:&#10;            missing.append(&quot;ADO_PAT&quot;)&#10;            print(&quot;[ERROR] ADO_PAT is not set&quot;)&#10;        else:&#10;            print(&quot;[CONFIG] ADO_PAT: [SECURED]&quot;)&#10;&#10;        if not cls.OPENAI_API_KEY:&#10;            missing.append(&quot;OPENAI_API_KEY&quot;)&#10;            print(&quot;[ERROR] OPENAI_API_KEY is not set&quot;)&#10;        else:&#10;            print(&quot;[CONFIG] OPENAI_API_KEY: [SECURED]&quot;)&#10;&#10;        if missing:&#10;            raise ValueError(&#10;                f&quot;Missing required environment variables: {', '.join(missing)}. &quot;&#10;                f&quot;Please check your .env file and ensure all required variables are set.&quot;&#10;            )&#10;" />
              <option name="updatedContent" value="import os&#10;from dotenv import load_dotenv&#10;&#10;class Settings:&#10;    &quot;&quot;&quot;Application settings loaded from environment variables&quot;&quot;&quot;&#10;&#10;    # Load environment variables&#10;    print(&quot;[CONFIG] Loading environment variables...&quot;)&#10;    load_dotenv()&#10;&#10;    # Azure DevOps settings&#10;    ADO_ORGANIZATION = os.getenv('ADO_ORGANIZATION')&#10;    ADO_PROJECT = os.getenv('ADO_PROJECT')&#10;    ADO_PAT = os.getenv('ADO_PAT')&#10;    ADO_BASE_URL = &quot;https://dev.azure.com&quot;&#10;&#10;    # Work item types&#10;    REQUIREMENT_TYPE = os.getenv('ADO_REQUIREMENT_TYPE', 'Epic')&#10;    USER_STORY_TYPE = os.getenv('ADO_USER_STORY_TYPE', 'User Story')&#10;&#10;    # OpenAI settings&#10;    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')&#10;    OPENAI_MAX_RETRIES = int(os.getenv('OPENAI_MAX_RETRIES', 3))&#10;    OPENAI_RETRY_DELAY = int(os.getenv('OPENAI_RETRY_DELAY', 5))&#10;&#10;    @classmethod&#10;    def validate(cls):&#10;        &quot;&quot;&quot;Validate required settings are present&quot;&quot;&quot;&#10;        print(&quot;[CONFIG] Validating settings...&quot;)&#10;        missing = []&#10;&#10;        # Check Azure DevOps settings&#10;        if not cls.ADO_ORGANIZATION:&#10;            missing.append(&quot;ADO_ORGANIZATION&quot;)&#10;            print(&quot;[ERROR] ADO_ORGANIZATION is not set&quot;)&#10;        else:&#10;            print(f&quot;[CONFIG] ADO_ORGANIZATION: {cls.ADO_ORGANIZATION}&quot;)&#10;&#10;        if not cls.ADO_PROJECT:&#10;            missing.append(&quot;ADO_PROJECT&quot;)&#10;            print(&quot;[ERROR] ADO_PROJECT is not set&quot;)&#10;        else:&#10;            print(f&quot;[CONFIG] ADO_PROJECT: {cls.ADO_PROJECT}&quot;)&#10;&#10;        if not cls.ADO_PAT:&#10;            missing.append(&quot;ADO_PAT&quot;)&#10;            print(&quot;[ERROR] ADO_PAT is not set&quot;)&#10;        else:&#10;            print(&quot;[CONFIG] ADO_PAT: [SECURED]&quot;)&#10;&#10;        if not cls.OPENAI_API_KEY:&#10;            missing.append(&quot;OPENAI_API_KEY&quot;)&#10;            print(&quot;[ERROR] OPENAI_API_KEY is not set&quot;)&#10;        else:&#10;            print(&quot;[CONFIG] OPENAI_API_KEY: [SECURED]&quot;)&#10;&#10;        if missing:&#10;            raise ValueError(&#10;                f&quot;Missing required environment variables: {', '.join(missing)}. &quot;&#10;                f&quot;Please check your .env file and ensure all required variables are set.&quot;&#10;            )" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/ado_client.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/ado_client.py" />
              <option name="originalContent" value="import base64&#10;import json&#10;import hashlib&#10;from typing import List, Optional, Dict, Any&#10;from datetime import datetime&#10;import requests&#10;from azure.devops.v7_1.work_item_tracking import WorkItemTrackingClient&#10;from msrest.authentication import BasicAuthentication&#10;&#10;from config.settings import Settings&#10;from src.models import Requirement, ExistingUserStory, RequirementSnapshot&#10;&#10;class ADOClient:&#10;    &quot;&quot;&quot;Client for interacting with Azure DevOps APIs&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        Settings.validate()&#10;        self.organization = Settings.ADO_ORGANIZATION&#10;        self.project = Settings.ADO_PROJECT&#10;        self.pat = Settings.ADO_PAT&#10;        self.base_url = f&quot;https://dev.azure.com/{self.organization}&quot;&#10;&#10;        try:&#10;            print(&quot;[DEBUG] Initializing work item tracking client...&quot;)&#10;            # Create credentials&#10;            credentials = BasicAuthentication('', self.pat)&#10;&#10;            # Create work item tracking client directly&#10;            self.wit_client = WorkItemTrackingClient(&#10;                base_url=self.base_url,&#10;                creds=credentials&#10;            )&#10;            print(&quot;[DEBUG] Work item tracking client created successfully&quot;)&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to establish connection to Azure DevOps: {str(e)}&quot;)&#10;&#10;    def get_requirements(self, state_filter: Optional[str] = None) -&gt; List[Requirement]:&#10;        &quot;&quot;&quot;Get all requirements from the project&quot;&quot;&quot;&#10;        try:&#10;            # Build WIQL query&#10;            wiql_query = f&quot;&quot;&quot;&#10;            SELECT [System.Id], [System.Title], [System.Description], [System.State]&#10;            FROM WorkItems&#10;            WHERE [System.WorkItemType] = '{Settings.REQUIREMENT_TYPE}'&#10;            AND [System.TeamProject] = '{self.project}'&#10;            &quot;&quot;&quot;&#10;            &#10;            if state_filter:&#10;                wiql_query += f&quot; AND [System.State] = '{state_filter}'&quot;&#10;            &#10;            # Execute query&#10;            wiql_result = self.wit_client.query_by_wiql({&quot;query&quot;: wiql_query})&#10;            &#10;            if not wiql_result.work_items:&#10;                return []&#10;            &#10;            # Get work item IDs&#10;            work_item_ids = [item.id for item in wiql_result.work_items]&#10;            &#10;            # Get full work items&#10;            work_items = self.wit_client.get_work_items(&#10;                ids=work_item_ids,&#10;                fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;]&#10;            )&#10;            &#10;            requirements = []&#10;            for item in work_items:&#10;                fields = item.fields&#10;                requirement = Requirement(&#10;                    id=item.id,&#10;                    title=fields.get(&quot;System.Title&quot;, &quot;&quot;),&#10;                    description=fields.get(&quot;System.Description&quot;, &quot;&quot;),&#10;                    state=fields.get(&quot;System.State&quot;, &quot;&quot;),&#10;                    url=item.url&#10;                )&#10;                requirements.append(requirement)&#10;            &#10;            return requirements&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get requirements: {str(e)}&quot;)&#10;    &#10;    def get_requirement_by_id(self, requirement_id: str) -&gt; Optional[Requirement]:&#10;        &quot;&quot;&quot;Get a single requirement by string ID with detailed error messages&quot;&quot;&quot;&#10;        try:&#10;            work_item = self.wit_client.get_work_item(id=requirement_id)&#10;            if not work_item:&#10;                print(f&quot;[ERROR] No work item found for ID: {requirement_id}&quot;)&#10;                return None&#10;            return Requirement.from_ado_work_item(work_item)&#10;        except Exception as e:&#10;            print(f&quot;[AUTH/ADO ERROR] Failed to fetch requirement '{requirement_id}': {e}.\n&quot;&#10;                  f&quot;Check if your PAT is valid, has correct permissions, and if the organization/project/ID are correct.&quot;)&#10;            return None&#10;&#10;    def create_user_story(self, story_data: Dict[str, Any], parent_requirement_id: int) -&gt; int:&#10;        &quot;&quot;&quot;Create a user story and link it to a parent requirement&quot;&quot;&quot;&#10;        try:&#10;            print(f&quot;[DEBUG] Attempting to create user story for parent {parent_requirement_id}&quot;)&#10;            # Prepare work item data - ensure we're using System.Title and System.Description&#10;            document = [&#10;                {&#10;                    &quot;op&quot;: &quot;add&quot;,&#10;                    &quot;path&quot;: &quot;/fields/System.Title&quot;,&#10;                    &quot;value&quot;: story_data.get(&quot;System.Title&quot;, &quot;New User Story&quot;)&#10;                },&#10;                {&#10;                    &quot;op&quot;: &quot;add&quot;,&#10;                    &quot;path&quot;: &quot;/fields/System.Description&quot;,&#10;                    &quot;value&quot;: story_data.get(&quot;System.Description&quot;, &quot;&quot;)&#10;                }&#10;            ]&#10;&#10;            # Add any additional fields from story_data&#10;            for field, value in story_data.items():&#10;                if field not in [&quot;System.Title&quot;, &quot;System.Description&quot;]:&#10;                    document.append({&#10;                        &quot;op&quot;: &quot;add&quot;,&#10;                        &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                        &quot;value&quot;: value&#10;                    })&#10;&#10;            print(f&quot;[DEBUG] Document prepared for Azure DevOps: {document}&quot;)&#10;&#10;            # Create the work item&#10;            try:&#10;                work_item = self.wit_client.create_work_item(&#10;                    document=document,&#10;                    project=self.project,&#10;                    type=Settings.USER_STORY_TYPE&#10;                )&#10;                print(f&quot;[DEBUG] Successfully created work item with ID: {work_item.id}&quot;)&#10;            except Exception as e:&#10;                print(f&quot;[ERROR] Failed to create work item: {str(e)}&quot;)&#10;                print(f&quot;[DEBUG] Project: {self.project}&quot;)&#10;                print(f&quot;[DEBUG] Type: {Settings.USER_STORY_TYPE}&quot;)&#10;                raise Exception(f&quot;Failed to create work item: {str(e)}&quot;)&#10;&#10;            # Create parent-child relationship if parent_requirement_id is provided&#10;            if parent_requirement_id:&#10;                try:&#10;                    print(f&quot;[DEBUG] Creating parent-child link between {parent_requirement_id} and {work_item.id}&quot;)&#10;                    self._create_parent_child_link(parent_requirement_id, work_item.id)&#10;                except Exception as e:&#10;                    print(f&quot;[WARNING] Failed to create parent-child link: {str(e)}&quot;)&#10;                    # Don't raise here, as the story was created successfully&#10;&#10;            return work_item.id&#10;            &#10;        except Exception as e:&#10;            print(f&quot;[ERROR] Error in create_user_story: {str(e)}&quot;)&#10;            raise&#10;&#10;    def _create_parent_child_link(self, parent_id: int, child_id: int):&#10;        &quot;&quot;&quot;Create a parent-child relationship between work items&quot;&quot;&quot;&#10;        try:&#10;            # Create the link&#10;            document = [{&#10;                &quot;op&quot;: &quot;add&quot;,&#10;                &quot;path&quot;: &quot;/relations/-&quot;,&#10;                &quot;value&quot;: {&#10;                    &quot;rel&quot;: &quot;System.LinkTypes.Hierarchy-Forward&quot;,&#10;                    &quot;url&quot;: f&quot;{self.base_url}/{self.organization}/_apis/wit/workItems/{child_id}&quot;&#10;                }&#10;            }]&#10;            &#10;            self.wit_client.update_work_item(&#10;                document=document,&#10;                id=parent_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to create parent-child link: {str(e)}&quot;)&#10;    &#10;    def detect_changes_in_epic(self, epic_id: int) -&gt; Optional[RequirementSnapshot]:&#10;        &quot;&quot;&quot;Detect changes in an EPIC based on its requirement snapshot&quot;&quot;&quot;&#10;        try:&#10;            work_item = self.wit_client.get_work_item(&#10;                id=epic_id,&#10;                fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;, &quot;System.ChangedDate&quot;]&#10;            )&#10;            &#10;            fields = work_item.fields&#10;            &#10;            # Calculate a hash of the title and description for change detection&#10;            content_hash = hashlib.sha256((fields[&quot;System.Title&quot;] + fields[&quot;System.Description&quot;]).encode()).hexdigest()&#10;            &#10;            return RequirementSnapshot(&#10;                id=work_item.id,&#10;                title=fields[&quot;System.Title&quot;],&#10;                description=fields[&quot;System.Description&quot;],&#10;                state=fields[&quot;System.State&quot;],&#10;                last_modified=datetime.strptime(fields[&quot;System.ChangedDate&quot;], &quot;%Y-%m-%dT%H:%M:%S.%fZ&quot;),&#10;                content_hash=content_hash&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to detect changes in EPIC {epic_id}: {str(e)}&quot;)&#10;&#10;    def get_existing_user_stories(self, epic_id: int) -&gt; List[ExistingUserStory]:&#10;        &quot;&quot;&quot;Retrieve existing user stories for a given epic ID&quot;&quot;&quot;&#10;        try:&#10;            child_ids = self.get_child_stories(epic_id)&#10;            stories = []&#10;            if child_ids:&#10;                work_items = self.wit_client.get_work_items(&#10;                    ids=child_ids,&#10;                    fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;]&#10;                )&#10;                for item in work_items:&#10;                    fields = item.fields&#10;                    story = ExistingUserStory(&#10;                        id=item.id,&#10;                        title=fields.get(&quot;System.Title&quot;, &quot;&quot;),&#10;                        description=fields.get(&quot;System.Description&quot;, &quot;&quot;),&#10;                        state=fields.get(&quot;System.State&quot;, &quot;&quot;),&#10;                        parent_id=epic_id&#10;                    )&#10;                    stories.append(story)&#10;            return stories&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to retrieve existing user stories for epic {epic_id}: {str(e)}&quot;)&#10;&#10;    def get_child_stories(self, requirement_id: int) -&gt; List[int]:&#10;        &quot;&quot;&quot;Get all child user stories for a requirement&quot;&quot;&quot;&#10;        try:&#10;            work_item = self.wit_client.get_work_item(&#10;                id=requirement_id,&#10;                expand=&quot;Relations&quot;&#10;            )&#10;            &#10;            child_ids = []&#10;            if work_item.relations:&#10;                for relation in work_item.relations:&#10;                    if relation.rel == &quot;System.LinkTypes.Hierarchy-Forward&quot;:&#10;                        # Extract work item ID from URL&#10;                        url_parts = relation.url.split('/')&#10;                        child_id = int(url_parts[-1])&#10;                        child_ids.append(child_id)&#10;            &#10;            return child_ids&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get child stories: {str(e)}&quot;)&#10;    &#10;    def update_work_item(self, work_item_id: int, update_data: Dict[str, Any]) -&gt; bool:&#10;        &quot;&quot;&quot;Update an existing work item&quot;&quot;&quot;&#10;        try:&#10;            # Prepare update document&#10;            document = []&#10;            for field, value in update_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.wit_client.update_work_item(&#10;                document=document,&#10;                id=work_item_id&#10;            )&#10;            &#10;            return True&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update work item {work_item_id}: {str(e)}&quot;)&#10;    &#10;    def get_work_item_types(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Get all available work item types in the project&quot;&quot;&quot;&#10;        try:&#10;            work_item_types = self.wit_client.get_work_item_types(project=self.project)&#10;            return [wit.name for wit in work_item_types]&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get work item types: {str(e)}&quot;)&#10;" />
              <option name="updatedContent" value="import base64&#10;import json&#10;import hashlib&#10;from typing import List, Optional, Dict, Any&#10;from datetime import datetime&#10;import requests&#10;from azure.devops.v7_1.work_item_tracking import WorkItemTrackingClient&#10;from msrest.authentication import BasicAuthentication&#10;&#10;from config.settings import Settings&#10;from src.models import Requirement, ExistingUserStory, RequirementSnapshot&#10;&#10;class ADOClient:&#10;    &quot;&quot;&quot;Client for interacting with Azure DevOps APIs&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        Settings.validate()&#10;        self.organization = Settings.ADO_ORGANIZATION&#10;        self.project = Settings.ADO_PROJECT&#10;        self.pat = Settings.ADO_PAT&#10;        self.base_url = f&quot;https://dev.azure.com/{self.organization}&quot;&#10;&#10;        try:&#10;            print(&quot;[DEBUG] Initializing work item tracking client...&quot;)&#10;            # Create credentials&#10;            credentials = BasicAuthentication('', self.pat)&#10;&#10;            # Create work item tracking client directly&#10;            self.wit_client = WorkItemTrackingClient(&#10;                base_url=self.base_url,&#10;                creds=credentials&#10;            )&#10;            print(&quot;[DEBUG] Work item tracking client created successfully&quot;)&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to establish connection to Azure DevOps: {str(e)}&quot;)&#10;&#10;    def get_requirements(self, state_filter: Optional[str] = None) -&gt; List[Requirement]:&#10;        &quot;&quot;&quot;Get all requirements from the project&quot;&quot;&quot;&#10;        try:&#10;            # Build WIQL query&#10;            wiql_query = f&quot;&quot;&quot;&#10;            SELECT [System.Id], [System.Title], [System.Description], [System.State]&#10;            FROM WorkItems&#10;            WHERE [System.WorkItemType] = '{Settings.REQUIREMENT_TYPE}'&#10;            AND [System.TeamProject] = '{self.project}'&#10;            &quot;&quot;&quot;&#10;            &#10;            if state_filter:&#10;                wiql_query += f&quot; AND [System.State] = '{state_filter}'&quot;&#10;            &#10;            # Execute query&#10;            wiql_result = self.wit_client.query_by_wiql({&quot;query&quot;: wiql_query})&#10;            &#10;            if not wiql_result.work_items:&#10;                return []&#10;            &#10;            # Get work item IDs&#10;            work_item_ids = [item.id for item in wiql_result.work_items]&#10;            &#10;            # Get full work items&#10;            work_items = self.wit_client.get_work_items(&#10;                ids=work_item_ids,&#10;                fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;]&#10;            )&#10;            &#10;            requirements = []&#10;            for item in work_items:&#10;                fields = item.fields&#10;                requirement = Requirement(&#10;                    id=item.id,&#10;                    title=fields.get(&quot;System.Title&quot;, &quot;&quot;),&#10;                    description=fields.get(&quot;System.Description&quot;, &quot;&quot;),&#10;                    state=fields.get(&quot;System.State&quot;, &quot;&quot;),&#10;                    url=item.url&#10;                )&#10;                requirements.append(requirement)&#10;            &#10;            return requirements&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get requirements: {str(e)}&quot;)&#10;    &#10;    def get_requirement_by_id(self, requirement_id: str) -&gt; Optional[Requirement]:&#10;        &quot;&quot;&quot;Get a single requirement by numeric ID or by title if not numeric&quot;&quot;&quot;&#10;        try:&#10;            # Try numeric lookup first&#10;            try:&#10;                numeric_id = int(requirement_id)&#10;                work_item = self.wit_client.get_work_item(id=numeric_id)&#10;                if not work_item:&#10;                    print(f&quot;[ERROR] No work item found for ID: {requirement_id}&quot;)&#10;                    return None&#10;                return Requirement.from_ado_work_item(work_item)&#10;            except ValueError:&#10;                # Not a numeric ID, search by title&#10;                print(f&quot;[INFO] Requirement ID '{requirement_id}' is not numeric. Searching by title...&quot;)&#10;                wiql_query = f&quot;&quot;&quot;&#10;                SELECT [System.Id], [System.Title], [System.Description], [System.State]&#10;                FROM WorkItems&#10;                WHERE [System.Title] = '{requirement_id}'&#10;                AND [System.TeamProject] = '{self.project}'&#10;                &quot;&quot;&quot;&#10;                wiql_result = self.wit_client.query_by_wiql({&quot;query&quot;: wiql_query})&#10;                if not wiql_result.work_items:&#10;                    print(f&quot;[ERROR] No work item found with title: {requirement_id}&quot;)&#10;                    return None&#10;                work_item_id = wiql_result.work_items[0].id&#10;                work_item = self.wit_client.get_work_item(id=work_item_id)&#10;                if not work_item:&#10;                    print(f&quot;[ERROR] No work item found for ID: {work_item_id}&quot;)&#10;                    return None&#10;                return Requirement.from_ado_work_item(work_item)&#10;        except Exception as e:&#10;            print(f&quot;[AUTH/ADO ERROR] Failed to fetch requirement '{requirement_id}': {e}.\n&quot;&#10;                  f&quot;Check if your PAT is valid, has correct permissions, and if the organization/project/ID are correct.&quot;)&#10;            return None&#10;&#10;    def create_user_story(self, story_data: Dict[str, Any], parent_requirement_id: int) -&gt; int:&#10;        &quot;&quot;&quot;Create a user story and link it to a parent requirement&quot;&quot;&quot;&#10;        try:&#10;            print(f&quot;[DEBUG] Attempting to create user story for parent {parent_requirement_id}&quot;)&#10;            # Prepare work item data - ensure we're using System.Title and System.Description&#10;            document = [&#10;                {&#10;                    &quot;op&quot;: &quot;add&quot;,&#10;                    &quot;path&quot;: &quot;/fields/System.Title&quot;,&#10;                    &quot;value&quot;: story_data.get(&quot;System.Title&quot;, &quot;New User Story&quot;)&#10;                },&#10;                {&#10;                    &quot;op&quot;: &quot;add&quot;,&#10;                    &quot;path&quot;: &quot;/fields/System.Description&quot;,&#10;                    &quot;value&quot;: story_data.get(&quot;System.Description&quot;, &quot;&quot;)&#10;                }&#10;            ]&#10;&#10;            # Add any additional fields from story_data&#10;            for field, value in story_data.items():&#10;                if field not in [&quot;System.Title&quot;, &quot;System.Description&quot;]:&#10;                    document.append({&#10;                        &quot;op&quot;: &quot;add&quot;,&#10;                        &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                        &quot;value&quot;: value&#10;                    })&#10;&#10;            print(f&quot;[DEBUG] Document prepared for Azure DevOps: {document}&quot;)&#10;&#10;            # Create the work item&#10;            try:&#10;                work_item = self.wit_client.create_work_item(&#10;                    document=document,&#10;                    project=self.project,&#10;                    type=Settings.USER_STORY_TYPE&#10;                )&#10;                print(f&quot;[DEBUG] Successfully created work item with ID: {work_item.id}&quot;)&#10;            except Exception as e:&#10;                print(f&quot;[ERROR] Failed to create work item: {str(e)}&quot;)&#10;                print(f&quot;[DEBUG] Project: {self.project}&quot;)&#10;                print(f&quot;[DEBUG] Type: {Settings.USER_STORY_TYPE}&quot;)&#10;                raise Exception(f&quot;Failed to create work item: {str(e)}&quot;)&#10;&#10;            # Create parent-child relationship if parent_requirement_id is provided&#10;            if parent_requirement_id:&#10;                try:&#10;                    print(f&quot;[DEBUG] Creating parent-child link between {parent_requirement_id} and {work_item.id}&quot;)&#10;                    self._create_parent_child_link(parent_requirement_id, work_item.id)&#10;                except Exception as e:&#10;                    print(f&quot;[WARNING] Failed to create parent-child link: {str(e)}&quot;)&#10;                    # Don't raise here, as the story was created successfully&#10;&#10;            return work_item.id&#10;            &#10;        except Exception as e:&#10;            print(f&quot;[ERROR] Error in create_user_story: {str(e)}&quot;)&#10;            raise&#10;&#10;    def _create_parent_child_link(self, parent_id: int, child_id: int):&#10;        &quot;&quot;&quot;Create a parent-child relationship between work items&quot;&quot;&quot;&#10;        try:&#10;            # Create the link&#10;            document = [{&#10;                &quot;op&quot;: &quot;add&quot;,&#10;                &quot;path&quot;: &quot;/relations/-&quot;,&#10;                &quot;value&quot;: {&#10;                    &quot;rel&quot;: &quot;System.LinkTypes.Hierarchy-Forward&quot;,&#10;                    &quot;url&quot;: f&quot;{self.base_url}/{self.organization}/_apis/wit/workItems/{child_id}&quot;&#10;                }&#10;            }]&#10;            &#10;            self.wit_client.update_work_item(&#10;                document=document,&#10;                id=parent_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to create parent-child link: {str(e)}&quot;)&#10;    &#10;    def detect_changes_in_epic(self, epic_id: int) -&gt; Optional[RequirementSnapshot]:&#10;        &quot;&quot;&quot;Detect changes in an EPIC based on its requirement snapshot&quot;&quot;&quot;&#10;        try:&#10;            work_item = self.wit_client.get_work_item(&#10;                id=epic_id,&#10;                fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;, &quot;System.ChangedDate&quot;]&#10;            )&#10;            &#10;            fields = work_item.fields&#10;            &#10;            # Calculate a hash of the title and description for change detection&#10;            content_hash = hashlib.sha256((fields[&quot;System.Title&quot;] + fields[&quot;System.Description&quot;]).encode()).hexdigest()&#10;            &#10;            return RequirementSnapshot(&#10;                id=work_item.id,&#10;                title=fields[&quot;System.Title&quot;],&#10;                description=fields[&quot;System.Description&quot;],&#10;                state=fields[&quot;System.State&quot;],&#10;                last_modified=datetime.strptime(fields[&quot;System.ChangedDate&quot;], &quot;%Y-%m-%dT%H:%M:%S.%fZ&quot;),&#10;                content_hash=content_hash&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to detect changes in EPIC {epic_id}: {str(e)}&quot;)&#10;&#10;    def get_existing_user_stories(self, epic_id: int) -&gt; List[ExistingUserStory]:&#10;        &quot;&quot;&quot;Retrieve existing user stories for a given epic ID&quot;&quot;&quot;&#10;        try:&#10;            child_ids = self.get_child_stories(epic_id)&#10;            stories = []&#10;            if child_ids:&#10;                work_items = self.wit_client.get_work_items(&#10;                    ids=child_ids,&#10;                    fields=[&quot;System.Id&quot;, &quot;System.Title&quot;, &quot;System.Description&quot;, &quot;System.State&quot;]&#10;                )&#10;                for item in work_items:&#10;                    fields = item.fields&#10;                    story = ExistingUserStory(&#10;                        id=item.id,&#10;                        title=fields.get(&quot;System.Title&quot;, &quot;&quot;),&#10;                        description=fields.get(&quot;System.Description&quot;, &quot;&quot;),&#10;                        state=fields.get(&quot;System.State&quot;, &quot;&quot;),&#10;                        parent_id=epic_id&#10;                    )&#10;                    stories.append(story)&#10;            return stories&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to retrieve existing user stories for epic {epic_id}: {str(e)}&quot;)&#10;&#10;    def get_child_stories(self, requirement_id: int) -&gt; List[int]:&#10;        &quot;&quot;&quot;Get all child user stories for a requirement&quot;&quot;&quot;&#10;        try:&#10;            work_item = self.wit_client.get_work_item(&#10;                id=requirement_id,&#10;                expand=&quot;Relations&quot;&#10;            )&#10;            &#10;            child_ids = []&#10;            if work_item.relations:&#10;                for relation in work_item.relations:&#10;                    if relation.rel == &quot;System.LinkTypes.Hierarchy-Forward&quot;:&#10;                        # Extract work item ID from URL&#10;                        url_parts = relation.url.split('/')&#10;                        child_id = int(url_parts[-1])&#10;                        child_ids.append(child_id)&#10;            &#10;            return child_ids&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get child stories: {str(e)}&quot;)&#10;    &#10;    def update_work_item(self, work_item_id: int, update_data: Dict[str, Any]) -&gt; bool:&#10;        &quot;&quot;&quot;Update an existing work item&quot;&quot;&quot;&#10;        try:&#10;            # Prepare update document&#10;            document = []&#10;            for field, value in update_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.wit_client.update_work_item(&#10;                document=document,&#10;                id=work_item_id&#10;            )&#10;            &#10;            return True&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update work item {work_item_id}: {str(e)}&quot;)&#10;    &#10;    def get_work_item_types(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Get all available work item types in the project&quot;&quot;&quot;&#10;        try:&#10;            work_item_types = self.wit_client.get_work_item_types(project=self.project)&#10;            return [wit.name for wit in work_item_types]&#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to get work item types: {str(e)}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/agent.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/agent.py" />
              <option name="originalContent" value="import logging&#10;from typing import List, Optional, Dict, Any&#10;&#10;from src.ado_client import ADOClient&#10;from src.story_extractor import StoryExtractor&#10;from src.models import Requirement, StoryExtractionResult, UserStory, ChangeDetectionResult, EpicSyncResult&#10;&#10;class StoryExtractionAgent:&#10;    &quot;&quot;&quot;Main agent that orchestrates the story extraction process&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.ado_client = ADOClient()&#10;        self.story_extractor = StoryExtractor()&#10;        self.logger = self._setup_logger()&#10;    &#10;    def process_requirement_by_id(self, requirement_id: str, upload_to_ado: bool = True) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Process a single requirement by ID&quot;&quot;&quot;&#10;        print(f&quot;\n[AGENT] Starting to process requirement ID: {requirement_id}&quot;)&#10;        try:&#10;            # Convert requirement_id to integer if it's a string&#10;            try:&#10;                numeric_id = int(requirement_id)&#10;                print(f&quot;[AGENT] Converted requirement ID to numeric: {numeric_id}&quot;)&#10;            except ValueError:&#10;                print(f&quot;[ERROR] Invalid requirement ID format: {requirement_id}. Must be a number.&quot;)&#10;                return StoryExtractionResult(&#10;                    requirement_id=requirement_id,&#10;                    requirement_title=&quot;&quot;,&#10;                    stories=[],&#10;                    extraction_successful=False,&#10;                    error_message=f&quot;Invalid requirement ID format: {requirement_id}. Must be a number.&quot;&#10;                )&#10;&#10;            # Get requirement from ADO&#10;            print(&quot;[AGENT] Fetching requirement from Azure DevOps...&quot;)&#10;            requirement = self.ado_client.get_requirement_by_id(str(numeric_id))&#10;&#10;            if not requirement:&#10;                error_msg = f&quot;Requirement {requirement_id} not found or access denied&quot;&#10;                print(f&quot;[ERROR] {error_msg}&quot;)&#10;                return StoryExtractionResult(&#10;                    requirement_id=requirement_id,&#10;                    requirement_title=&quot;&quot;,&#10;                    stories=[],&#10;                    extraction_successful=False,&#10;                    error_message=error_msg&#10;                )&#10;&#10;            print(f&quot;[AGENT] Found requirement: {requirement.title}&quot;)&#10;&#10;            # Extract stories&#10;            print(&quot;[DEBUG] StoryExtractionAgent: Starting story extraction&quot;)&#10;            result = self.story_extractor.extract_stories(requirement)&#10;            &#10;            if not result.extraction_successful:&#10;                print(f&quot;[ERROR] StoryExtractionAgent: Story extraction failed: {result.error_message}&quot;)&#10;                return result&#10;            &#10;            print(f&quot;[DEBUG] StoryExtractionAgent: Successfully extracted {len(result.stories)} stories&quot;)&#10;&#10;            # Upload to ADO if requested&#10;            if upload_to_ado and result.stories:&#10;                print(&quot;[DEBUG] StoryExtractionAgent: Starting upload to ADO&quot;)&#10;                try:&#10;                    uploaded_story_ids = self._upload_stories_to_ado(result.stories, requirement_id)&#10;                    print(f&quot;[DEBUG] StoryExtractionAgent: Successfully uploaded {len(uploaded_story_ids)} stories&quot;)&#10;                except Exception as e:&#10;                    print(f&quot;[ERROR] StoryExtractionAgent: Failed to upload stories: {str(e)}&quot;)&#10;                    result.error_message = f&quot;Failed to upload stories: {str(e)}&quot;&#10;                    result.extraction_successful = False&#10;&#10;            return result&#10;            &#10;        except Exception as e:&#10;            error_msg = f&quot;Failed to process requirement {requirement_id}: {str(e)}&quot;&#10;            print(f&quot;[ERROR] StoryExtractionAgent: {error_msg}&quot;)&#10;            return StoryExtractionResult(&#10;                requirement_id=requirement_id,&#10;                requirement_title=&quot;&quot;,&#10;                stories=[],&#10;                extraction_successful=False,&#10;                error_message=error_msg&#10;            )&#10;    &#10;    def process_all_requirements(self, state_filter: Optional[str] = None, upload_to_ado: bool = True) -&gt; List[StoryExtractionResult]:&#10;        &quot;&quot;&quot;Process all requirements in the project&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Processing all requirements with state filter: {state_filter}&quot;)&#10;        &#10;        try:&#10;            # Get all requirements&#10;            requirements = self.ado_client.get_requirements(state_filter)&#10;            self.logger.info(f&quot;Found {len(requirements)} requirements to process&quot;)&#10;            &#10;            results = []&#10;            for requirement in requirements:&#10;                result = self.process_requirement_by_id(str(requirement.id), upload_to_ado)&#10;                results.append(result)&#10;            &#10;            # Summary&#10;            successful = len([r for r in results if r.extraction_successful])&#10;            total_stories = sum(len(r.stories) for r in results)&#10;            self.logger.info(f&quot;Processing complete: {successful}/{len(results)} requirements processed successfully, {total_stories} total stories extracted&quot;)&#10;            &#10;            return results&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to process requirements: {str(e)}&quot;)&#10;            raise&#10;    &#10;    def preview_stories(self, requirement_id: str) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Extract and preview stories without uploading to ADO&quot;&quot;&quot;&#10;        return self.process_requirement_by_id(requirement_id, upload_to_ado=False)&#10;    &#10;    def _upload_stories_to_ado(self, stories: List[UserStory], parent_requirement_id: str) -&gt; List[int]:&#10;        &quot;&quot;&quot;Upload user stories to ADO as child items of the requirement&quot;&quot;&quot;&#10;        parent_id = parent_requirement_id  # No numeric parsing anymore&#10;        uploaded_ids = []&#10;        &#10;        for story in stories:&#10;            try:&#10;                story_data = story.to_ado_format()&#10;                story_id = self.ado_client.create_user_story(story_data, parent_id)&#10;                uploaded_ids.append(story_id)&#10;                self.logger.info(f&quot;Created user story {story_id}: {story.heading}&quot;)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Failed to create user story '{story.heading}': {str(e)}&quot;)&#10;                continue&#10;        &#10;        return uploaded_ids&#10;    &#10;    def get_requirement_summary(self, requirement_id: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Get a summary of a requirement and its child stories&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = requirement_id  # No numeric parsing&#10;            requirement = self.ado_client.get_requirement_by_id(numeric_id)&#10;            if not requirement:&#10;                return {&quot;error&quot;: f&quot;Requirement {requirement_id} not found&quot;}&#10;            &#10;            child_story_ids = self.ado_client.get_child_stories(numeric_id)&#10;            &#10;            return {&#10;                &quot;requirement&quot;: {&#10;                    &quot;id&quot;: requirement.id,&#10;                    &quot;title&quot;: requirement.title,&#10;                    &quot;description&quot;: requirement.description[:200] + &quot;...&quot; if len(requirement.description) &gt; 200 else requirement.description,&#10;                    &quot;state&quot;: requirement.state&#10;                },&#10;                &quot;child_stories&quot;: {&#10;                    &quot;count&quot;: len(child_story_ids),&#10;                    &quot;ids&quot;: child_story_ids&#10;                }&#10;            }&#10;            &#10;        except Exception as e:&#10;            return {&quot;error&quot;: str(e)}&#10;    &#10;    def synchronize_epic(self, epic_id: str, stored_snapshot: Optional[Dict[str, str]] = None) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Detect changes in an EPIC and synchronize its tasks&quot;&quot;&quot;&#10;        try:&#10;            self.logger.info(f&quot;Synchronizing EPIC {epic_id}&quot;)&#10;            &#10;            numeric_epic_id = epic_id  # No numeric parsing&#10;&#10;            # Get current EPIC state&#10;            current_epic = self.ado_client.get_requirement_by_id(epic_id)&#10;            if not current_epic:&#10;                raise Exception(f&quot;EPIC {epic_id} not found&quot;)&#10;            &#10;            # Get current snapshot for change detection&#10;            current_snapshot = self.ado_client.detect_changes_in_epic(numeric_epic_id)&#10;            existing_stories = self.ado_client.get_existing_user_stories(numeric_epic_id)&#10;            &#10;            self.logger.info(f&quot;Found {len(existing_stories)} existing user stories for EPIC {epic_id}&quot;)&#10;            &#10;            # Check if changes detected (compare with stored snapshot if provided)&#10;            has_changes = True  # Always extract stories for now, can be refined later&#10;            changes_detected = []&#10;            &#10;            if stored_snapshot and current_snapshot:&#10;                stored_hash = stored_snapshot.get('content_hash', '')&#10;                if stored_hash == current_snapshot.content_hash:&#10;                    has_changes = False&#10;                    self.logger.info(f&quot;No content changes detected for EPIC {epic_id}&quot;)&#10;                else:&#10;                    changes_detected.append(&quot;EPIC content has been modified&quot;)&#10;                    self.logger.info(f&quot;Content changes detected for EPIC {epic_id}&quot;)&#10;            else:&#10;                changes_detected.append(&quot;Initial sync or no previous snapshot available&quot;)&#10;            &#10;            sync_result = EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=current_epic.title&#10;            )&#10;            &#10;            if has_changes:&#10;                # Extract new stories from the updated EPIC&#10;                story_extraction_result = self.story_extractor.extract_stories(current_epic)&#10;                &#10;                if not story_extraction_result.extraction_successful:&#10;                    raise Exception(f&quot;Story extraction failed: {story_extraction_result.error_message}&quot;)&#10;                &#10;                new_stories = story_extraction_result.stories&#10;                self.logger.info(f&quot;Extracted {len(new_stories)} stories from updated EPIC&quot;)&#10;                &#10;                # Determine which stories to create, update, or leave unchanged&#10;                stories_to_create, stories_to_update, unchanged_stories = self._analyze_story_changes(&#10;                    existing_stories, new_stories&#10;                )&#10;                &#10;                self.logger.info(f&quot;Analysis: {len(stories_to_create)} to create, {len(stories_to_update)} to update, {len(unchanged_stories)} unchanged&quot;)&#10;                &#10;                # Create new stories&#10;                created_ids = []&#10;                for story in stories_to_create:&#10;                    try:&#10;                        story_data = story.to_ado_format()&#10;                        story_id = self.ado_client.create_user_story(story_data, numeric_epic_id)&#10;                        created_ids.append(story_id)&#10;                        self.logger.info(f&quot;Created new user story {story_id}: {story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to create story '{story.heading}': {str(e)}&quot;)&#10;                        continue&#10;                &#10;                # Update existing stories&#10;                updated_ids = []&#10;                for story_update in stories_to_update:&#10;                    try:&#10;                        story_id = story_update['id']&#10;                        new_story = story_update['new_story']&#10;                        self._update_user_story(story_id, new_story)&#10;                        updated_ids.append(story_id)&#10;                        self.logger.info(f&quot;Updated user story {story_id}: {new_story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to update story {story_update['id']}: {str(e)}&quot;)&#10;                        continue&#10;                &#10;                sync_result.created_stories = created_ids&#10;                sync_result.updated_stories = updated_ids&#10;                sync_result.unchanged_stories = [s.id for s in unchanged_stories]&#10;                &#10;                self.logger.info(f&quot;EPIC sync completed: {len(created_ids)} created, {len(updated_ids)} updated, {len(unchanged_stories)} unchanged&quot;)&#10;            else:&#10;                sync_result.unchanged_stories = [s.id for s in existing_stories]&#10;                self.logger.info(f&quot;No changes detected, {len(existing_stories)} stories remain unchanged&quot;)&#10;            &#10;            return sync_result&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;EPIC synchronization failed for {epic_id}: {str(e)}&quot;)&#10;            return EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=&quot;&quot;,&#10;                sync_successful=False,&#10;                error_message=str(e)&#10;            )&#10;    &#10;    def _analyze_story_changes(self, existing_stories, new_stories):&#10;        &quot;&quot;&quot;Analyze differences between existing and new stories to determine what to create/update&quot;&quot;&quot;&#10;        from difflib import SequenceMatcher&#10;        &#10;        stories_to_create = []&#10;        stories_to_update = []&#10;        unchanged_stories = []&#10;        &#10;        # Convert existing stories to a dict for easier lookup&#10;        existing_by_title = {story.title: story for story in existing_stories}&#10;        &#10;        # Check each new story against existing ones&#10;        for new_story in new_stories:&#10;            best_match = None&#10;            best_similarity = 0.0&#10;            &#10;            # Find the best matching existing story by title similarity&#10;            for existing_title, existing_story in existing_by_title.items():&#10;                similarity = SequenceMatcher(None, new_story.heading.lower(), existing_title.lower()).ratio()&#10;                if similarity &gt; best_similarity:&#10;                    best_similarity = similarity&#10;                    best_match = existing_story&#10;            &#10;            # If we found a good match (similarity &gt; 0.8), consider it for update&#10;            if best_match and best_similarity &gt; 0.8:&#10;                # Check if the content has actually changed&#10;                existing_content = f&quot;{best_match.title} {best_match.description}&quot;&#10;                new_content = f&quot;{new_story.heading} {new_story.description} {' '.join(new_story.acceptance_criteria)}&quot;&#10;                &#10;                content_similarity = SequenceMatcher(None, existing_content.lower(), new_content.lower()).ratio()&#10;                &#10;                if content_similarity &lt; 0.9:  # Content has changed significantly&#10;                    stories_to_update.append({&#10;                        'id': best_match.id,&#10;                        'existing_story': best_match,&#10;                        'new_story': new_story&#10;                    })&#10;                    # Remove from existing dict so it's not considered again&#10;                    del existing_by_title[best_match.title]&#10;                else:&#10;                    unchanged_stories.append(best_match)&#10;                    del existing_by_title[best_match.title]&#10;            else:&#10;                # No good match found, this is a new story&#10;                stories_to_create.append(new_story)&#10;        &#10;        # Any remaining existing stories that weren't matched are considered unchanged&#10;        for remaining_story in existing_by_title.values():&#10;            unchanged_stories.append(remaining_story)&#10;        &#10;        return stories_to_create, stories_to_update, unchanged_stories&#10;    &#10;    def _update_user_story(self, story_id: int, new_story: UserStory):&#10;        &quot;&quot;&quot;Update an existing user story in ADO&quot;&quot;&quot;&#10;        try:&#10;            story_data = new_story.to_ado_format()&#10;            &#10;            # Prepare update document&#10;            document = []&#10;            for field, value in story_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.ado_client.wit_client.update_work_item(&#10;                document=document,&#10;                id=story_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update user story {story_id}: {str(e)}&quot;)&#10;    &#10;    def get_epic_snapshot(self, epic_id: str) -&gt; Optional[Dict[str, str]]:&#10;        &quot;&quot;&quot;Get a snapshot of the current EPIC for change tracking&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = epic_id  # No numeric parsing&#10;            snapshot = self.ado_client.detect_changes_in_epic(numeric_id)&#10;            &#10;            if snapshot:&#10;                return {&#10;                    'content_hash': snapshot.content_hash,&#10;                    'last_modified': snapshot.last_modified.isoformat() if snapshot.last_modified else None,&#10;                    'title': snapshot.title,&#10;                    'state': snapshot.state&#10;                }&#10;            return None&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get EPIC snapshot for {epic_id}: {str(e)}&quot;)&#10;            return None&#10;&#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging configuration&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;StoryExtractionAgent&quot;)&#10;        logger.setLevel(logging.INFO)&#10;        &#10;        if not logger.handlers:&#10;            handler = logging.StreamHandler()&#10;            formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            handler.setFormatter(formatter)&#10;            logger.addHandler(handler)&#10;        &#10;        return logger&#10;" />
              <option name="updatedContent" value="import logging&#10;from typing import List, Optional, Dict, Any&#10;&#10;from src.ado_client import ADOClient&#10;from src.story_extractor import StoryExtractor&#10;from src.models import Requirement, StoryExtractionResult, UserStory, ChangeDetectionResult, EpicSyncResult&#10;&#10;class StoryExtractionAgent:&#10;    &quot;&quot;&quot;Main agent that orchestrates the story extraction process&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.ado_client = ADOClient()&#10;        self.story_extractor = StoryExtractor()&#10;        self.logger = self._setup_logger()&#10;    &#10;    def process_requirement_by_id(self, requirement_id: str, upload_to_ado: bool = True) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Process a single requirement by ID (now supports string IDs)&quot;&quot;&quot;&#10;        print(f&quot;\n[AGENT] Starting to process requirement ID: {requirement_id}&quot;)&#10;        try:&#10;            # Accept string-based IDs (e.g., 'EPIC 1')&#10;            ado_id = requirement_id.strip()&#10;            print(f&quot;[AGENT] Using requirement ID: {ado_id}&quot;)&#10;&#10;            # Get requirement from ADO&#10;            print(&quot;[AGENT] Fetching requirement from Azure DevOps...&quot;)&#10;            requirement = self.ado_client.get_requirement_by_id(ado_id)&#10;&#10;            if not requirement:&#10;                error_msg = f&quot;Requirement {requirement_id} not found or access denied&quot;&#10;                print(f&quot;[ERROR] {error_msg}&quot;)&#10;                return StoryExtractionResult(&#10;                    requirement_id=requirement_id,&#10;                    requirement_title=&quot;&quot;,&#10;                    stories=[],&#10;                    extraction_successful=False,&#10;                    error_message=error_msg&#10;                )&#10;&#10;            print(f&quot;[AGENT] Found requirement: {requirement.title}&quot;)&#10;&#10;            # Extract stories&#10;            print(&quot;[DEBUG] StoryExtractionAgent: Starting story extraction&quot;)&#10;            result = self.story_extractor.extract_stories(requirement)&#10;            &#10;            if not result.extraction_successful:&#10;                print(f&quot;[ERROR] StoryExtractionAgent: Story extraction failed: {result.error_message}&quot;)&#10;                return result&#10;            &#10;            print(f&quot;[DEBUG] StoryExtractionAgent: Successfully extracted {len(result.stories)} stories&quot;)&#10;&#10;            # Upload to ADO if requested&#10;            if upload_to_ado and result.stories:&#10;                print(&quot;[DEBUG] StoryExtractionAgent: Starting upload to ADO&quot;)&#10;                try:&#10;                    uploaded_story_ids = self._upload_stories_to_ado(result.stories, requirement_id)&#10;                    print(f&quot;[DEBUG] StoryExtractionAgent: Successfully uploaded {len(uploaded_story_ids)} stories&quot;)&#10;                except Exception as e:&#10;                    print(f&quot;[ERROR] StoryExtractionAgent: Failed to upload stories: {str(e)}&quot;)&#10;                    result.error_message = f&quot;Failed to upload stories: {str(e)}&quot;&#10;                    result.extraction_successful = False&#10;&#10;            return result&#10;            &#10;        except Exception as e:&#10;            error_msg = f&quot;Failed to process requirement {requirement_id}: {str(e)}&quot;&#10;            print(f&quot;[ERROR] StoryExtractionAgent: {error_msg}&quot;)&#10;            return StoryExtractionResult(&#10;                requirement_id=requirement_id,&#10;                requirement_title=&quot;&quot;,&#10;                stories=[],&#10;                extraction_successful=False,&#10;                error_message=error_msg&#10;            )&#10;    &#10;    def process_all_requirements(self, state_filter: Optional[str] = None, upload_to_ado: bool = True) -&gt; List[StoryExtractionResult]:&#10;        &quot;&quot;&quot;Process all requirements in the project&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Processing all requirements with state filter: {state_filter}&quot;)&#10;        &#10;        try:&#10;            # Get all requirements&#10;            requirements = self.ado_client.get_requirements(state_filter)&#10;            self.logger.info(f&quot;Found {len(requirements)} requirements to process&quot;)&#10;            &#10;            results = []&#10;            for requirement in requirements:&#10;                result = self.process_requirement_by_id(str(requirement.id), upload_to_ado)&#10;                results.append(result)&#10;            &#10;            # Summary&#10;            successful = len([r for r in results if r.extraction_successful])&#10;            total_stories = sum(len(r.stories) for r in results)&#10;            self.logger.info(f&quot;Processing complete: {successful}/{len(results)} requirements processed successfully, {total_stories} total stories extracted&quot;)&#10;            &#10;            return results&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to process requirements: {str(e)}&quot;)&#10;            raise&#10;    &#10;    def preview_stories(self, requirement_id: str) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Extract and preview stories without uploading to ADO&quot;&quot;&quot;&#10;        return self.process_requirement_by_id(requirement_id, upload_to_ado=False)&#10;    &#10;    def _upload_stories_to_ado(self, stories: List[UserStory], parent_requirement_id: str) -&gt; List[int]:&#10;        &quot;&quot;&quot;Upload user stories to ADO as child items of the requirement&quot;&quot;&quot;&#10;        parent_id = parent_requirement_id  # No numeric parsing anymore&#10;        uploaded_ids = []&#10;        &#10;        for story in stories:&#10;            try:&#10;                story_data = story.to_ado_format()&#10;                story_id = self.ado_client.create_user_story(story_data, parent_id)&#10;                uploaded_ids.append(story_id)&#10;                self.logger.info(f&quot;Created user story {story_id}: {story.heading}&quot;)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Failed to create user story '{story.heading}': {str(e)}&quot;)&#10;                continue&#10;        &#10;        return uploaded_ids&#10;    &#10;    def get_requirement_summary(self, requirement_id: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Get a summary of a requirement and its child stories&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = requirement_id  # No numeric parsing&#10;            requirement = self.ado_client.get_requirement_by_id(numeric_id)&#10;            if not requirement:&#10;                return {&quot;error&quot;: f&quot;Requirement {requirement_id} not found&quot;}&#10;            &#10;            child_story_ids = self.ado_client.get_child_stories(numeric_id)&#10;            &#10;            return {&#10;                &quot;requirement&quot;: {&#10;                    &quot;id&quot;: requirement.id,&#10;                    &quot;title&quot;: requirement.title,&#10;                    &quot;description&quot;: requirement.description[:200] + &quot;...&quot; if len(requirement.description) &gt; 200 else requirement.description,&#10;                    &quot;state&quot;: requirement.state&#10;                },&#10;                &quot;child_stories&quot;: {&#10;                    &quot;count&quot;: len(child_story_ids),&#10;                    &quot;ids&quot;: child_story_ids&#10;                }&#10;            }&#10;            &#10;        except Exception as e:&#10;            return {&quot;error&quot;: str(e)}&#10;    &#10;    def synchronize_epic(self, epic_id: str, stored_snapshot: Optional[Dict[str, str]] = None) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Detect changes in an EPIC and synchronize its tasks&quot;&quot;&quot;&#10;        try:&#10;            self.logger.info(f&quot;Synchronizing EPIC {epic_id}&quot;)&#10;            &#10;            numeric_epic_id = epic_id  # No numeric parsing&#10;&#10;            # Get current EPIC state&#10;            current_epic = self.ado_client.get_requirement_by_id(epic_id)&#10;            if not current_epic:&#10;                raise Exception(f&quot;EPIC {epic_id} not found&quot;)&#10;            &#10;            # Get current snapshot for change detection&#10;            current_snapshot = self.ado_client.detect_changes_in_epic(numeric_epic_id)&#10;            existing_stories = self.ado_client.get_existing_user_stories(numeric_epic_id)&#10;            &#10;            self.logger.info(f&quot;Found {len(existing_stories)} existing user stories for EPIC {epic_id}&quot;)&#10;            &#10;            # Check if changes detected (compare with stored snapshot if provided)&#10;            has_changes = True  # Always extract stories for now, can be refined later&#10;            changes_detected = []&#10;            &#10;            if stored_snapshot and current_snapshot:&#10;                stored_hash = stored_snapshot.get('content_hash', '')&#10;                if stored_hash == current_snapshot.content_hash:&#10;                    has_changes = False&#10;                    self.logger.info(f&quot;No content changes detected for EPIC {epic_id}&quot;)&#10;                else:&#10;                    changes_detected.append(&quot;EPIC content has been modified&quot;)&#10;                    self.logger.info(f&quot;Content changes detected for EPIC {epic_id}&quot;)&#10;            else:&#10;                changes_detected.append(&quot;Initial sync or no previous snapshot available&quot;)&#10;            &#10;            sync_result = EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=current_epic.title&#10;            )&#10;            &#10;            if has_changes:&#10;                # Extract new stories from the updated EPIC&#10;                story_extraction_result = self.story_extractor.extract_stories(current_epic)&#10;                &#10;                if not story_extraction_result.extraction_successful:&#10;                    raise Exception(f&quot;Story extraction failed: {story_extraction_result.error_message}&quot;)&#10;                &#10;                new_stories = story_extraction_result.stories&#10;                self.logger.info(f&quot;Extracted {len(new_stories)} stories from updated EPIC&quot;)&#10;                &#10;                # Determine which stories to create, update, or leave unchanged&#10;                stories_to_create, stories_to_update, unchanged_stories = self._analyze_story_changes(&#10;                    existing_stories, new_stories&#10;                )&#10;                &#10;                self.logger.info(f&quot;Analysis: {len(stories_to_create)} to create, {len(stories_to_update)} to update, {len(unchanged_stories)} unchanged&quot;)&#10;                &#10;                # Create new stories&#10;                created_ids = []&#10;                for story in stories_to_create:&#10;                    try:&#10;                        story_data = story.to_ado_format()&#10;                        story_id = self.ado_client.create_user_story(story_data, numeric_epic_id)&#10;                        created_ids.append(story_id)&#10;                        self.logger.info(f&quot;Created new user story {story_id}: {story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to create story '{story.heading}': {str(e)}&quot;)&#10;                        continue&#10;                &#10;                # Update existing stories&#10;                updated_ids = []&#10;                for story_update in stories_to_update:&#10;                    try:&#10;                        story_id = story_update['id']&#10;                        new_story = story_update['new_story']&#10;                        self._update_user_story(story_id, new_story)&#10;                        updated_ids.append(story_id)&#10;                        self.logger.info(f&quot;Updated user story {story_id}: {new_story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to update story {story_update['id']}: {str(e)}&quot;)&#10;                        continue&#10;                &#10;                sync_result.created_stories = created_ids&#10;                sync_result.updated_stories = updated_ids&#10;                sync_result.unchanged_stories = [s.id for s in unchanged_stories]&#10;                &#10;                self.logger.info(f&quot;EPIC sync completed: {len(created_ids)} created, {len(updated_ids)} updated, {len(unchanged_stories)} unchanged&quot;)&#10;            else:&#10;                sync_result.unchanged_stories = [s.id for s in existing_stories]&#10;                self.logger.info(f&quot;No changes detected, {len(existing_stories)} stories remain unchanged&quot;)&#10;            &#10;            return sync_result&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;EPIC synchronization failed for {epic_id}: {str(e)}&quot;)&#10;            return EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=&quot;&quot;,&#10;                sync_successful=False,&#10;                error_message=str(e)&#10;            )&#10;    &#10;    def _analyze_story_changes(self, existing_stories, new_stories):&#10;        &quot;&quot;&quot;Analyze differences between existing and new stories to determine what to create/update&quot;&quot;&quot;&#10;        from difflib import SequenceMatcher&#10;        &#10;        stories_to_create = []&#10;        stories_to_update = []&#10;        unchanged_stories = []&#10;        &#10;        # Convert existing stories to a dict for easier lookup&#10;        existing_by_title = {story.title: story for story in existing_stories}&#10;        &#10;        # Check each new story against existing ones&#10;        for new_story in new_stories:&#10;            best_match = None&#10;            best_similarity = 0.0&#10;            &#10;            # Find the best matching existing story by title similarity&#10;            for existing_title, existing_story in existing_by_title.items():&#10;                similarity = SequenceMatcher(None, new_story.heading.lower(), existing_title.lower()).ratio()&#10;                if similarity &gt; best_similarity:&#10;                    best_similarity = similarity&#10;                    best_match = existing_story&#10;            &#10;            # If we found a good match (similarity &gt; 0.8), consider it for update&#10;            if best_match and best_similarity &gt; 0.8:&#10;                # Check if the content has actually changed&#10;                existing_content = f&quot;{best_match.title} {best_match.description}&quot;&#10;                new_content = f&quot;{new_story.heading} {new_story.description} {' '.join(new_story.acceptance_criteria)}&quot;&#10;                &#10;                content_similarity = SequenceMatcher(None, existing_content.lower(), new_content.lower()).ratio()&#10;                &#10;                if content_similarity &lt; 0.9:  # Content has changed significantly&#10;                    stories_to_update.append({&#10;                        'id': best_match.id,&#10;                        'existing_story': best_match,&#10;                        'new_story': new_story&#10;                    })&#10;                    # Remove from existing dict so it's not considered again&#10;                    del existing_by_title[best_match.title]&#10;                else:&#10;                    unchanged_stories.append(best_match)&#10;                    del existing_by_title[best_match.title]&#10;            else:&#10;                # No good match found, this is a new story&#10;                stories_to_create.append(new_story)&#10;        &#10;        # Any remaining existing stories that weren't matched are considered unchanged&#10;        for remaining_story in existing_by_title.values():&#10;            unchanged_stories.append(remaining_story)&#10;        &#10;        return stories_to_create, stories_to_update, unchanged_stories&#10;    &#10;    def _update_user_story(self, story_id: int, new_story: UserStory):&#10;        &quot;&quot;&quot;Update an existing user story in ADO&quot;&quot;&quot;&#10;        try:&#10;            story_data = new_story.to_ado_format()&#10;            &#10;            # Prepare update document&#10;            document = []&#10;            for field, value in story_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.ado_client.wit_client.update_work_item(&#10;                document=document,&#10;                id=story_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update user story {story_id}: {str(e)}&quot;)&#10;    &#10;    def get_epic_snapshot(self, epic_id: str) -&gt; Optional[Dict[str, str]]:&#10;        &quot;&quot;&quot;Get a snapshot of the current EPIC for change tracking&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = epic_id  # No numeric parsing&#10;            snapshot = self.ado_client.detect_changes_in_epic(numeric_id)&#10;            &#10;            if snapshot:&#10;                return {&#10;                    'content_hash': snapshot.content_hash,&#10;                    'last_modified': snapshot.last_modified.isoformat() if snapshot.last_modified else None,&#10;                    'title': snapshot.title,&#10;                    'state': snapshot.state&#10;                }&#10;            return None&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get EPIC snapshot for {epic_id}: {str(e)}&quot;)&#10;            return None&#10;&#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging configuration&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;StoryExtractionAgent&quot;)&#10;        logger.setLevel(logging.INFO)&#10;        &#10;        if not logger.handlers:&#10;            handler = logging.StreamHandler()&#10;            formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            handler.setFormatter(formatter)&#10;            logger.addHandler(handler)&#10;        &#10;        return logger" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/models.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/models.py" />
              <option name="originalContent" value="from typing import List, Optional, Dict, Any&#10;from datetime import datetime&#10;from pydantic import BaseModel, Field&#10;&#10;class UserStory(BaseModel):&#10;    &quot;&quot;&quot;Model representing a user story extracted from a requirement&quot;&quot;&quot;&#10;    heading: str = Field(..., description=&quot;Title/heading of the user story&quot;)&#10;    description: str = Field(..., description=&quot;Detailed description of what the user wants&quot;)&#10;    acceptance_criteria: List[str] = Field(..., description=&quot;List of acceptance criteria&quot;)&#10;    &#10;    def to_ado_format(self) -&gt; dict:&#10;        &quot;&quot;&quot;Convert to Azure DevOps work item format with acceptance criteria in description&quot;&quot;&quot;&#10;        # Format acceptance criteria as HTML bulleted list for proper ADO display&#10;        acceptance_criteria_html = &quot;&lt;br&gt;&quot;.join([f&quot;• {criteria}&quot; for criteria in self.acceptance_criteria])&#10;        &#10;        # Combine description with acceptance criteria using HTML formatting&#10;        # ADO description field expects HTML format for proper newline rendering&#10;        full_description = f&quot;{self.description}&lt;br&gt;&lt;br&gt;&lt;strong&gt;Acceptance Criteria:&lt;/strong&gt;&lt;br&gt;{acceptance_criteria_html}&quot;&#10;        &#10;        return {&#10;            &quot;System.Title&quot;: self.heading,&#10;            &quot;System.Description&quot;: full_description&#10;        }&#10;&#10;class Requirement(BaseModel):&#10;    &quot;&quot;&quot;Model representing an ADO requirement&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    url: Optional[str] = None&#10;    &#10;class StoryExtractionResult(BaseModel):&#10;    &quot;&quot;&quot;Result of story extraction from a requirement&quot;&quot;&quot;&#10;    requirement_id: str  # Changed to str to handle both numeric and text IDs&#10;    requirement_title: str&#10;    stories: List[UserStory]&#10;    extraction_successful: bool = True&#10;    error_message: Optional[str] = None&#10;&#10;class ExistingUserStory(BaseModel):&#10;    &quot;&quot;&quot;Model representing an existing user story in ADO&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    parent_id: Optional[int] = None&#10;    &#10;class ChangeDetectionResult(BaseModel):&#10;    &quot;&quot;&quot;Result of change detection for an EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    epic_title: str&#10;    has_changes: bool = False&#10;    changes_detected: List[str] = Field(default_factory=list)&#10;    last_modified: Optional[datetime] = None&#10;    existing_stories: List[ExistingUserStory] = Field(default_factory=list)&#10;    new_stories: List[UserStory] = Field(default_factory=list)&#10;    stories_to_update: List[Dict[str, Any]] = Field(default_factory=list)&#10;    stories_to_create: List[UserStory] = Field(default_factory=list)&#10;    &#10;class EpicSyncResult(BaseModel):&#10;    &quot;&quot;&quot;Result of synchronizing an EPIC with its user stories&quot;&quot;&quot;&#10;    epic_id: str&#10;    epic_title: str&#10;    sync_successful: bool = True&#10;    created_stories: List[int] = Field(default_factory=list)&#10;    updated_stories: List[int] = Field(default_factory=list)&#10;    unchanged_stories: List[int] = Field(default_factory=list)&#10;    error_message: Optional[str] = None&#10;    &#10;class RequirementSnapshot(BaseModel):&#10;    &quot;&quot;&quot;Snapshot of a requirement for change tracking&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    last_modified: Optional[datetime] = None&#10;    content_hash: Optional[str] = None  # Hash of title + description for quick comparison&#10;" />
              <option name="updatedContent" value="from typing import List, Optional, Dict, Any&#10;from datetime import datetime&#10;from pydantic import BaseModel, Field&#10;&#10;class UserStory(BaseModel):&#10;    &quot;&quot;&quot;Model representing a user story extracted from a requirement&quot;&quot;&quot;&#10;    heading: str = Field(..., description=&quot;Title/heading of the user story&quot;)&#10;    description: str = Field(..., description=&quot;Detailed description of what the user wants&quot;)&#10;    acceptance_criteria: List[str] = Field(..., description=&quot;List of acceptance criteria&quot;)&#10;    &#10;    def to_ado_format(self) -&gt; dict:&#10;        &quot;&quot;&quot;Convert to Azure DevOps work item format with acceptance criteria in description&quot;&quot;&quot;&#10;        # Format acceptance criteria as HTML bulleted list for proper ADO display&#10;        acceptance_criteria_html = &quot;&lt;br&gt;&quot;.join([f&quot;• {criteria}&quot; for criteria in self.acceptance_criteria])&#10;        &#10;        # Combine description with acceptance criteria using HTML formatting&#10;        # ADO description field expects HTML format for proper newline rendering&#10;        full_description = f&quot;{self.description}&lt;br&gt;&lt;br&gt;&lt;strong&gt;Acceptance Criteria:&lt;/strong&gt;&lt;br&gt;{acceptance_criteria_html}&quot;&#10;        &#10;        return {&#10;            &quot;System.Title&quot;: self.heading,&#10;            &quot;System.Description&quot;: full_description&#10;        }&#10;&#10;class Requirement(BaseModel):&#10;    &quot;&quot;&quot;Model representing an ADO requirement&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    url: Optional[str] = None&#10;    &#10;    @staticmethod&#10;    def from_ado_work_item(work_item: Any) -&gt; &quot;Requirement&quot;:&#10;        &quot;&quot;&quot;Create a Requirement instance from an Azure DevOps work item object.&quot;&quot;&quot;&#10;        fields = getattr(work_item, 'fields', {})&#10;        return Requirement(&#10;            id=getattr(work_item, 'id', 0),&#10;            title=fields.get(&quot;System.Title&quot;, &quot;&quot;),&#10;            description=fields.get(&quot;System.Description&quot;, &quot;&quot;),&#10;            state=fields.get(&quot;System.State&quot;, &quot;&quot;),&#10;            url=getattr(work_item, 'url', None)&#10;        )&#10;&#10;class StoryExtractionResult(BaseModel):&#10;    &quot;&quot;&quot;Result of story extraction from a requirement&quot;&quot;&quot;&#10;    requirement_id: str  # Changed to str to handle both numeric and text IDs&#10;    requirement_title: str&#10;    stories: List[UserStory]&#10;    extraction_successful: bool = True&#10;    error_message: Optional[str] = None&#10;&#10;class ExistingUserStory(BaseModel):&#10;    &quot;&quot;&quot;Model representing an existing user story in ADO&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    parent_id: Optional[int] = None&#10;    &#10;class ChangeDetectionResult(BaseModel):&#10;    &quot;&quot;&quot;Result of change detection for an EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    epic_title: str&#10;    has_changes: bool = False&#10;    changes_detected: List[str] = Field(default_factory=list)&#10;    last_modified: Optional[datetime] = None&#10;    existing_stories: List[ExistingUserStory] = Field(default_factory=list)&#10;    new_stories: List[UserStory] = Field(default_factory=list)&#10;    stories_to_update: List[Dict[str, Any]] = Field(default_factory=list)&#10;    stories_to_create: List[UserStory] = Field(default_factory=list)&#10;    &#10;class EpicSyncResult(BaseModel):&#10;    &quot;&quot;&quot;Result of synchronizing an EPIC with its user stories&quot;&quot;&quot;&#10;    epic_id: str&#10;    epic_title: str&#10;    sync_successful: bool = True&#10;    created_stories: List[int] = Field(default_factory=list)&#10;    updated_stories: List[int] = Field(default_factory=list)&#10;    unchanged_stories: List[int] = Field(default_factory=list)&#10;    error_message: Optional[str] = None&#10;    &#10;class RequirementSnapshot(BaseModel):&#10;    &quot;&quot;&quot;Snapshot of a requirement for change tracking&quot;&quot;&quot;&#10;    id: int&#10;    title: str&#10;    description: str&#10;    state: str&#10;    last_modified: Optional[datetime] = None&#10;    content_hash: Optional[str] = None  # Hash of title + description for quick comparison" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/monitor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/monitor.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;Background monitoring service for EPIC change detection and automatic synchronization.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import json&#10;import logging&#10;import os&#10;import signal&#10;import sys&#10;import time&#10;from datetime import datetime, timedelta&#10;from pathlib import Path&#10;from typing import Dict, List, Optional, Set&#10;from dataclasses import dataclass, asdict&#10;from concurrent.futures import ThreadPoolExecutor&#10;&#10;from src.agent import StoryExtractionAgent&#10;from src.models import EpicSyncResult, RequirementSnapshot&#10;from config.settings import Settings&#10;&#10;&#10;@dataclass&#10;class MonitorConfig:&#10;    &quot;&quot;&quot;Configuration for the EPIC monitor&quot;&quot;&quot;&#10;    poll_interval_seconds: int = 300  # 5 minutes default&#10;    max_concurrent_syncs: int = 3&#10;    snapshot_directory: str = &quot;snapshots&quot;&#10;    log_level: str = &quot;INFO&quot;&#10;    epic_ids: List[str] = None&#10;    auto_sync: bool = True&#10;    notification_webhook: Optional[str] = None&#10;    retry_attempts: int = 3&#10;    retry_delay_seconds: int = 60&#10;&#10;&#10;@dataclass&#10;class EpicMonitorState:&#10;    &quot;&quot;&quot;State tracking for a monitored EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    last_check: datetime&#10;    last_snapshot: Optional[Dict] = None&#10;    consecutive_errors: int = 0&#10;    last_sync_result: Optional[Dict] = None&#10;&#10;&#10;class EpicChangeMonitor:&#10;    &quot;&quot;&quot;Background service that monitors EPICs for changes and triggers synchronization&quot;&quot;&quot;&#10;    &#10;    def __init__(self, config: MonitorConfig):&#10;        self.config = config&#10;        self.agent = StoryExtractionAgent()&#10;        self.logger = self._setup_logger()&#10;        self.is_running = False&#10;        self.monitored_epics: Dict[str, EpicMonitorState] = {}&#10;        self.executor = ThreadPoolExecutor(max_workers=config.max_concurrent_syncs)&#10;        &#10;        # Ensure snapshot directory exists&#10;        self.snapshot_dir = Path(config.snapshot_directory)&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;        &#10;        # Load existing snapshots&#10;        self._load_existing_snapshots()&#10;    &#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging for the monitor&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;EpicChangeMonitor&quot;)&#10;        logger.setLevel(getattr(logging, self.config.log_level.upper()))&#10;        &#10;        if not logger.handlers:&#10;            # Console handler&#10;            console_handler = logging.StreamHandler()&#10;            console_formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            console_handler.setFormatter(console_formatter)&#10;            logger.addHandler(console_handler)&#10;            &#10;            # File handler&#10;            log_file = Path(&quot;logs&quot;) / &quot;epic_monitor.log&quot;&#10;            log_file.parent.mkdir(exist_ok=True)&#10;            file_handler = logging.FileHandler(log_file)&#10;            file_handler.setFormatter(console_formatter)&#10;            logger.addHandler(file_handler)&#10;        &#10;        return logger&#10;    &#10;    def _load_existing_snapshots(self):&#10;        &quot;&quot;&quot;Load existing snapshots for monitored EPICs&quot;&quot;&quot;&#10;        for epic_id in self.config.epic_ids or []:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            if snapshot_file.exists():&#10;                try:&#10;                    with open(snapshot_file, 'r') as f:&#10;                        snapshot_data = json.load(f)&#10;                    &#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=snapshot_data&#10;                    )&#10;                    self.logger.info(f&quot;Loaded existing snapshot for EPIC {epic_id}&quot;)&#10;                except Exception as e:&#10;                    self.logger.error(f&quot;Failed to load snapshot for EPIC {epic_id}: {e}&quot;)&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now()&#10;                    )&#10;            else:&#10;                self.monitored_epics[epic_id] = EpicMonitorState(&#10;                    epic_id=epic_id,&#10;                    last_check=datetime.now()&#10;                )&#10;    &#10;    def add_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Add an EPIC to monitoring&quot;&quot;&quot;&#10;        try:&#10;            if epic_id not in self.monitored_epics:&#10;                # Get initial snapshot&#10;                initial_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                if initial_snapshot:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=initial_snapshot&#10;                    )&#10;                    self._save_snapshot(epic_id, initial_snapshot)&#10;                    self.logger.info(f&quot;Added EPIC {epic_id} to monitoring&quot;)&#10;                    return True&#10;                else:&#10;                    self.logger.error(f&quot;Failed to get initial snapshot for EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                self.logger.warning(f&quot;EPIC {epic_id} is already being monitored&quot;)&#10;                return True&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to add EPIC {epic_id} to monitoring: {e}&quot;)&#10;            return False&#10;    &#10;    def remove_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Remove an EPIC from monitoring&quot;&quot;&quot;&#10;        if epic_id in self.monitored_epics:&#10;            del self.monitored_epics[epic_id]&#10;            self.logger.info(f&quot;Removed EPIC {epic_id} from monitoring&quot;)&#10;            return True&#10;        return False&#10;    &#10;    def _save_snapshot(self, epic_id: str, snapshot: Dict):&#10;        &quot;&quot;&quot;Save snapshot to file&quot;&quot;&quot;&#10;        try:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            with open(snapshot_file, 'w') as f:&#10;                json.dump(snapshot, f, indent=2)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save snapshot for EPIC {epic_id}: {e}&quot;)&#10;    &#10;    def _check_epic_changes(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Check if an EPIC has changes&quot;&quot;&quot;&#10;        try:&#10;            epic_state = self.monitored_epics[epic_id]&#10;            current_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;            &#10;            if not current_snapshot:&#10;                self.logger.warning(f&quot;Failed to get current snapshot for EPIC {epic_id}&quot;)&#10;                epic_state.consecutive_errors += 1&#10;                return False&#10;            &#10;            # Reset error counter on successful snapshot&#10;            epic_state.consecutive_errors = 0&#10;            &#10;            # Compare with last known snapshot&#10;            if epic_state.last_snapshot:&#10;                last_hash = epic_state.last_snapshot.get('content_hash', '')&#10;                current_hash = current_snapshot.get('content_hash', '')&#10;                &#10;                if last_hash != current_hash:&#10;                    self.logger.info(f&quot;Changes detected in EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Previous hash: {last_hash[:16]}...&quot;)&#10;                    self.logger.info(f&quot;  Current hash:  {current_hash[:16]}...&quot;)&#10;                    return True&#10;                else:&#10;                    self.logger.debug(f&quot;No changes detected in EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                # First check, save current snapshot&#10;                self.logger.info(f&quot;Initial snapshot saved for EPIC {epic_id}&quot;)&#10;                epic_state.last_snapshot = current_snapshot&#10;                self._save_snapshot(epic_id, current_snapshot)&#10;                return False&#10;                &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Error checking changes for EPIC {epic_id}: {e}&quot;)&#10;            self.monitored_epics[epic_id].consecutive_errors += 1&#10;            return False&#10;    &#10;    def _sync_epic(self, epic_id: str) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Synchronize an EPIC with retry logic&quot;&quot;&quot;&#10;        epic_state = self.monitored_epics[epic_id]&#10;        &#10;        for attempt in range(self.config.retry_attempts):&#10;            try:&#10;                self.logger.info(f&quot;Synchronizing EPIC {epic_id} (attempt {attempt + 1})&quot;)&#10;                &#10;                result = self.agent.synchronize_epic(&#10;                    epic_id=epic_id,&#10;                    stored_snapshot=epic_state.last_snapshot&#10;                )&#10;                &#10;                if result.sync_successful:&#10;                    # Update snapshot after successful sync&#10;                    new_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                    if new_snapshot:&#10;                        epic_state.last_snapshot = new_snapshot&#10;                        self._save_snapshot(epic_id, new_snapshot)&#10;                    &#10;                    # Store sync result&#10;                    epic_state.last_sync_result = {&#10;                        'timestamp': datetime.now().isoformat(),&#10;                        'success': True,&#10;                        'created_stories': result.created_stories,&#10;                        'updated_stories': result.updated_stories,&#10;                        'unchanged_stories': result.unchanged_stories&#10;                    }&#10;                    &#10;                    self.logger.info(f&quot;Successfully synchronized EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Created: {len(result.created_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Updated: {len(result.updated_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Unchanged: {len(result.unchanged_stories)} stories&quot;)&#10;                    &#10;                    return result&#10;                else:&#10;                    self.logger.error(f&quot;Sync failed for EPIC {epic_id}: {result.error_message}&quot;)&#10;                    if attempt &lt; self.config.retry_attempts - 1:&#10;                        self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                        time.sleep(self.config.retry_delay_seconds)&#10;                    &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Exception during sync of EPIC {epic_id}: {e}&quot;)&#10;                if attempt &lt; self.config.retry_attempts - 1:&#10;                    self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                    time.sleep(self.config.retry_delay_seconds)&#10;        &#10;        # All attempts failed&#10;        epic_state.last_sync_result = {&#10;            'timestamp': datetime.now().isoformat(),&#10;            'success': False,&#10;            'error': f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        }&#10;        &#10;        return EpicSyncResult(&#10;            epic_id=epic_id,&#10;            epic_title=&quot;&quot;,&#10;            sync_successful=False,&#10;            error_message=f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        )&#10;    &#10;    async def _monitor_loop(self):&#10;        &quot;&quot;&quot;Main monitoring loop&quot;&quot;&quot;&#10;        self.logger.info(&quot;Starting EPIC monitoring loop&quot;)&#10;        &#10;        while self.is_running:&#10;            try:&#10;                # Check each monitored EPIC&#10;                sync_tasks = []&#10;                &#10;                for epic_id in list(self.monitored_epics.keys()):&#10;                    try:&#10;                        epic_state = self.monitored_epics[epic_id]&#10;                        &#10;                        # Skip if too many consecutive errors&#10;                        if epic_state.consecutive_errors &gt;= 5:&#10;                            self.logger.warning(f&quot;Skipping EPIC {epic_id} due to consecutive errors&quot;)&#10;                            continue&#10;                        &#10;                        # Check for changes&#10;                        if self._check_epic_changes(epic_id):&#10;                            if self.config.auto_sync:&#10;                                # Schedule sync&#10;                                future = asyncio.get_event_loop().run_in_executor(&#10;                                    self.executor, self._sync_epic, epic_id&#10;                                )&#10;                                sync_tasks.append((epic_id, future))&#10;                            else:&#10;                                self.logger.info(f&quot;Changes detected in EPIC {epic_id}, but auto-sync is disabled&quot;)&#10;                        &#10;                        # Update last check time&#10;                        epic_state.last_check = datetime.now()&#10;                        &#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Error processing EPIC {epic_id}: {e}&quot;)&#10;                        import traceback&#10;                        self.logger.error(traceback.format_exc())&#10;&#10;                # Wait for sync tasks to complete&#10;                if sync_tasks:&#10;                    self.logger.info(f&quot;Running {len(sync_tasks)} synchronization tasks&quot;)&#10;                    for epic_id, future in sync_tasks:&#10;                        try:&#10;                            await future&#10;                        except Exception as e:&#10;                            self.logger.error(f&quot;Sync task failed for EPIC {epic_id}: {e}&quot;)&#10;                            import traceback&#10;                            self.logger.error(traceback.format_exc())&#10;&#10;                # Wait before next polling cycle&#10;                self.logger.debug(f&quot;Monitoring cycle complete, sleeping for {self.config.poll_interval_seconds} seconds&quot;)&#10;                await asyncio.sleep(self.config.poll_interval_seconds)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Error in monitoring loop: {e}&quot;)&#10;                import traceback&#10;                self.logger.error(traceback.format_exc())&#10;                await asyncio.sleep(60)  # Wait a minute before retrying&#10;    &#10;    def fetch_all_epic_ids(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Fetch all Epic IDs from Azure DevOps.&quot;&quot;&quot;&#10;        try:&#10;            requirements = self.agent.ado_client.get_requirements()&#10;            return [str(req.id) for req in requirements]&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to fetch all Epics: {e}&quot;)&#10;            return []&#10;&#10;    def update_monitored_epics(self):&#10;        &quot;&quot;&quot;Update the monitored Epics set by auto-detecting new Epics.&quot;&quot;&quot;&#10;        all_epic_ids = set(self.fetch_all_epic_ids())&#10;        current_epic_ids = set(self.monitored_epics.keys())&#10;        new_epics = all_epic_ids - current_epic_ids&#10;        for epic_id in new_epics:&#10;            self.logger.info(f&quot;Auto-detect: Adding new Epic {epic_id} to monitoring.&quot;)&#10;            self.add_epic(epic_id)&#10;        # Optionally, remove Epics that no longer exist in ADO&#10;        # removed_epics = current_epic_ids - all_epic_ids&#10;        # for epic_id in removed_epics:&#10;        #     self.logger.info(f&quot;Auto-detect: Removing Epic {epic_id} (no longer exists in ADO).&quot;)&#10;        #     self.monitored_epics.pop(epic_id, None)&#10;&#10;    def start(self):&#10;        &quot;&quot;&quot;Start the monitoring service&quot;&quot;&quot;&#10;        if self.is_running:&#10;            self.logger.warning(&quot;Monitor is already running&quot;)&#10;            return&#10;        &#10;        self.is_running = True&#10;        self.logger.info(&quot;Starting EPIC Change Monitor&quot;)&#10;        self.logger.info(f&quot;Monitoring {len(self.monitored_epics)} EPICs&quot;)&#10;        self.logger.info(f&quot;Poll interval: {self.config.poll_interval_seconds} seconds&quot;)&#10;        self.logger.info(f&quot;Auto-sync enabled: {self.config.auto_sync}&quot;)&#10;        &#10;        # Setup signal handlers for graceful shutdown&#10;        signal.signal(signal.SIGINT, self._signal_handler)&#10;        signal.signal(signal.SIGTERM, self._signal_handler)&#10;        &#10;        # Run the monitoring loop&#10;        try:&#10;            asyncio.run(self._monitor_loop())&#10;        except KeyboardInterrupt:&#10;            self.logger.info(&quot;Received interrupt signal&quot;)&#10;        finally:&#10;            self.stop()&#10;    &#10;    def stop(self):&#10;        &quot;&quot;&quot;Stop the monitoring service&quot;&quot;&quot;&#10;        if not self.is_running:&#10;            return&#10;        &#10;        self.logger.info(&quot;Stopping EPIC Change Monitor&quot;)&#10;        self.is_running = False&#10;        self.executor.shutdown(wait=True)&#10;        self.logger.info(&quot;EPIC Change Monitor stopped&quot;)&#10;    &#10;    def _signal_handler(self, signum, frame):&#10;        &quot;&quot;&quot;Handle shutdown signals&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Received signal {signum}, shutting down gracefully...&quot;)&#10;        self.stop()&#10;        sys.exit(0)&#10;    &#10;    def get_status(self) -&gt; Dict:&#10;        &quot;&quot;&quot;Get current monitoring status&quot;&quot;&quot;&#10;        status = {&#10;            'is_running': self.is_running,&#10;            'config': asdict(self.config),&#10;            'monitored_epics': {},&#10;            'last_update': datetime.now().isoformat()&#10;        }&#10;        &#10;        for epic_id, state in self.monitored_epics.items():&#10;            status['monitored_epics'][epic_id] = {&#10;                'last_check': state.last_check.isoformat(),&#10;                'consecutive_errors': state.consecutive_errors,&#10;                'has_snapshot': state.last_snapshot is not None,&#10;                'last_sync_result': state.last_sync_result&#10;            }&#10;        &#10;        return status&#10;    &#10;    def force_check(self, epic_id: Optional[str] = None) -&gt; Dict:&#10;        &quot;&quot;&quot;Force a check for changes (optionally for specific EPIC)&quot;&quot;&quot;&#10;        results = {}&#10;        &#10;        epics_to_check = [epic_id] if epic_id else list(self.monitored_epics.keys())&#10;        &#10;        for eid in epics_to_check:&#10;            if eid in self.monitored_epics:&#10;                try:&#10;                    has_changes = self._check_epic_changes(eid)&#10;                    results[eid] = {&#10;                        'has_changes': has_changes,&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;                    &#10;                    if has_changes and self.config.auto_sync:&#10;                        sync_result = self._sync_epic(eid)&#10;                        results[eid]['sync_result'] = {&#10;                            'success': sync_result.sync_successful,&#10;                            'created_stories': sync_result.created_stories,&#10;                            'updated_stories': sync_result.updated_stories,&#10;                            'error_message': sync_result.error_message&#10;                        }&#10;                except Exception as e:&#10;                    results[eid] = {&#10;                        'error': str(e),&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;        &#10;        return results&#10;&#10;&#10;def load_config_from_file(config_file: str) -&gt; MonitorConfig:&#10;    &quot;&quot;&quot;Load monitor configuration from JSON file&quot;&quot;&quot;&#10;    try:&#10;        with open(config_file, 'r') as f:&#10;            config_data = json.load(f)&#10;        return MonitorConfig(**config_data)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to load config from {config_file}: {e}&quot;)&#10;        return MonitorConfig()&#10;&#10;&#10;def create_default_config(config_file: str = &quot;monitor_config.json&quot;):&#10;    &quot;&quot;&quot;Create a default configuration file&quot;&quot;&quot;&#10;    default_config = MonitorConfig(&#10;        poll_interval_seconds=300,  # 5 minutes&#10;        max_concurrent_syncs=3,&#10;        snapshot_directory=&quot;snapshots&quot;,&#10;        log_level=&quot;INFO&quot;,&#10;        epic_ids=[&quot;12345&quot;, &quot;67890&quot;],  # Example EPIC IDs&#10;        auto_sync=True,&#10;        retry_attempts=3,&#10;        retry_delay_seconds=60&#10;    )&#10;    &#10;    with open(config_file, 'w') as f:&#10;        json.dump(asdict(default_config), f, indent=2)&#10;    &#10;    print(f&quot;Created default configuration file: {config_file}&quot;)&#10;    return default_config&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;Background monitoring service for EPIC change detection and automatic synchronization.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import json&#10;import logging&#10;import os&#10;import signal&#10;import sys&#10;import time&#10;from datetime import datetime, timedelta&#10;from pathlib import Path&#10;from typing import Dict, List, Optional, Set&#10;from dataclasses import dataclass, asdict&#10;from concurrent.futures import ThreadPoolExecutor&#10;&#10;from src.agent import StoryExtractionAgent&#10;from src.models import EpicSyncResult, RequirementSnapshot&#10;from config.settings import Settings&#10;&#10;&#10;@dataclass&#10;class MonitorConfig:&#10;    &quot;&quot;&quot;Configuration for the EPIC monitor&quot;&quot;&quot;&#10;    poll_interval_seconds: int = 300  # 5 minutes default&#10;    max_concurrent_syncs: int = 3&#10;    snapshot_directory: str = &quot;snapshots&quot;&#10;    log_level: str = &quot;INFO&quot;&#10;    epic_ids: List[str] = None&#10;    auto_sync: bool = True&#10;    notification_webhook: Optional[str] = None&#10;    retry_attempts: int = 3&#10;    retry_delay_seconds: int = 60&#10;&#10;&#10;@dataclass&#10;class EpicMonitorState:&#10;    &quot;&quot;&quot;State tracking for a monitored EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    last_check: datetime&#10;    last_snapshot: Optional[Dict] = None&#10;    consecutive_errors: int = 0&#10;    last_sync_result: Optional[Dict] = None&#10;&#10;&#10;class EpicChangeMonitor:&#10;    &quot;&quot;&quot;Background service that monitors EPICs for changes and triggers synchronization&quot;&quot;&quot;&#10;    &#10;    def __init__(self, config: MonitorConfig):&#10;        self.config = config&#10;        self.agent = StoryExtractionAgent()&#10;        self.logger = self._setup_logger()&#10;        self.is_running = False&#10;        self.monitored_epics: Dict[str, EpicMonitorState] = {}&#10;        self.executor = ThreadPoolExecutor(max_workers=config.max_concurrent_syncs)&#10;        &#10;        # Ensure snapshot directory exists&#10;        self.snapshot_dir = Path(config.snapshot_directory)&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;        &#10;        # Load existing snapshots&#10;        self._load_existing_snapshots()&#10;    &#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging for the monitor&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;EpicChangeMonitor&quot;)&#10;        logger.setLevel(getattr(logging, self.config.log_level.upper()))&#10;        &#10;        if not logger.handlers:&#10;            # Console handler&#10;            console_handler = logging.StreamHandler()&#10;            console_formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            console_handler.setFormatter(console_formatter)&#10;            logger.addHandler(console_handler)&#10;            &#10;            # File handler&#10;            log_file = Path(&quot;logs&quot;) / &quot;epic_monitor.log&quot;&#10;            log_file.parent.mkdir(exist_ok=True)&#10;            file_handler = logging.FileHandler(log_file)&#10;            file_handler.setFormatter(console_formatter)&#10;            logger.addHandler(file_handler)&#10;        &#10;        return logger&#10;    &#10;    def _load_existing_snapshots(self):&#10;        &quot;&quot;&quot;Load existing snapshots for monitored EPICs&quot;&quot;&quot;&#10;        for epic_id in self.config.epic_ids or []:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            if snapshot_file.exists():&#10;                try:&#10;                    with open(snapshot_file, 'r') as f:&#10;                        snapshot_data = json.load(f)&#10;                    &#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=snapshot_data&#10;                    )&#10;                    self.logger.info(f&quot;Loaded existing snapshot for EPIC {epic_id}&quot;)&#10;                except Exception as e:&#10;                    self.logger.error(f&quot;Failed to load snapshot for EPIC {epic_id}: {e}&quot;)&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now()&#10;                    )&#10;            else:&#10;                self.monitored_epics[epic_id] = EpicMonitorState(&#10;                    epic_id=epic_id,&#10;                    last_check=datetime.now()&#10;                )&#10;    &#10;    def add_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Add an EPIC to monitoring&quot;&quot;&quot;&#10;        try:&#10;            if epic_id not in self.monitored_epics:&#10;                # Get initial snapshot&#10;                initial_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                if initial_snapshot:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=initial_snapshot&#10;                    )&#10;                    self._save_snapshot(epic_id, initial_snapshot)&#10;                    self.logger.info(f&quot;Added EPIC {epic_id} to monitoring&quot;)&#10;                    return True&#10;                else:&#10;                    self.logger.error(f&quot;Failed to get initial snapshot for EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                self.logger.warning(f&quot;EPIC {epic_id} is already being monitored&quot;)&#10;                return True&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to add EPIC {epic_id} to monitoring: {e}&quot;)&#10;            return False&#10;    &#10;    def remove_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Remove an EPIC from monitoring&quot;&quot;&quot;&#10;        if epic_id in self.monitored_epics:&#10;            del self.monitored_epics[epic_id]&#10;            self.logger.info(f&quot;Removed EPIC {epic_id} from monitoring&quot;)&#10;            return True&#10;        return False&#10;    &#10;    def _save_snapshot(self, epic_id: str, snapshot: Dict):&#10;        &quot;&quot;&quot;Save snapshot to file&quot;&quot;&quot;&#10;        try:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            with open(snapshot_file, 'w') as f:&#10;                json.dump(snapshot, f, indent=2)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save snapshot for EPIC {epic_id}: {e}&quot;)&#10;    &#10;    def _check_epic_changes(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Check if an EPIC has changes&quot;&quot;&quot;&#10;        try:&#10;            epic_state = self.monitored_epics[epic_id]&#10;            current_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;            &#10;            if not current_snapshot:&#10;                self.logger.warning(f&quot;Failed to get current snapshot for EPIC {epic_id}&quot;)&#10;                epic_state.consecutive_errors += 1&#10;                return False&#10;            &#10;            # Reset error counter on successful snapshot&#10;            epic_state.consecutive_errors = 0&#10;            &#10;            # Compare with last known snapshot&#10;            if epic_state.last_snapshot:&#10;                last_hash = epic_state.last_snapshot.get('content_hash', '')&#10;                current_hash = current_snapshot.get('content_hash', '')&#10;                &#10;                if last_hash != current_hash:&#10;                    self.logger.info(f&quot;Changes detected in EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Previous hash: {last_hash[:16]}...&quot;)&#10;                    self.logger.info(f&quot;  Current hash:  {current_hash[:16]}...&quot;)&#10;                    return True&#10;                else:&#10;                    self.logger.debug(f&quot;No changes detected in EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                # First check, save current snapshot&#10;                self.logger.info(f&quot;Initial snapshot saved for EPIC {epic_id}&quot;)&#10;                epic_state.last_snapshot = current_snapshot&#10;                self._save_snapshot(epic_id, current_snapshot)&#10;                return False&#10;                &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Error checking changes for EPIC {epic_id}: {e}&quot;)&#10;            self.monitored_epics[epic_id].consecutive_errors += 1&#10;            return False&#10;    &#10;    def _sync_epic(self, epic_id: str) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Synchronize an EPIC with retry logic&quot;&quot;&quot;&#10;        epic_state = self.monitored_epics[epic_id]&#10;        &#10;        for attempt in range(self.config.retry_attempts):&#10;            try:&#10;                self.logger.info(f&quot;Synchronizing EPIC {epic_id} (attempt {attempt + 1})&quot;)&#10;                &#10;                result = self.agent.synchronize_epic(&#10;                    epic_id=epic_id,&#10;                    stored_snapshot=epic_state.last_snapshot&#10;                )&#10;                &#10;                if result.sync_successful:&#10;                    # Update snapshot after successful sync&#10;                    new_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                    if new_snapshot:&#10;                        epic_state.last_snapshot = new_snapshot&#10;                        self._save_snapshot(epic_id, new_snapshot)&#10;                    &#10;                    # Store sync result&#10;                    epic_state.last_sync_result = {&#10;                        'timestamp': datetime.now().isoformat(),&#10;                        'success': True,&#10;                        'created_stories': result.created_stories,&#10;                        'updated_stories': result.updated_stories,&#10;                        'unchanged_stories': result.unchanged_stories&#10;                    }&#10;                    &#10;                    self.logger.info(f&quot;Successfully synchronized EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Created: {len(result.created_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Updated: {len(result.updated_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Unchanged: {len(result.unchanged_stories)} stories&quot;)&#10;                    &#10;                    return result&#10;                else:&#10;                    self.logger.error(f&quot;Sync failed for EPIC {epic_id}: {result.error_message}&quot;)&#10;                    if attempt &lt; self.config.retry_attempts - 1:&#10;                        self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                        time.sleep(self.config.retry_delay_seconds)&#10;                    &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Exception during sync of EPIC {epic_id}: {e}&quot;)&#10;                if attempt &lt; self.config.retry_attempts - 1:&#10;                    self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                    time.sleep(self.config.retry_delay_seconds)&#10;        &#10;        # All attempts failed&#10;        epic_state.last_sync_result = {&#10;            'timestamp': datetime.now().isoformat(),&#10;            'success': False,&#10;            'error': f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        }&#10;        &#10;        return EpicSyncResult(&#10;            epic_id=epic_id,&#10;            epic_title=&quot;&quot;,&#10;            sync_successful=False,&#10;            error_message=f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        )&#10;    &#10;    async def _monitor_loop(self):&#10;        &quot;&quot;&quot;Main monitoring loop&quot;&quot;&quot;&#10;        self.logger.info(&quot;Starting EPIC monitoring loop&quot;)&#10;        &#10;        while self.is_running:&#10;            try:&#10;                # Auto-detect new Epics at the start of each cycle&#10;                self.update_monitored_epics()&#10;                # Check each monitored EPIC&#10;                sync_tasks = []&#10;                &#10;                for epic_id in list(self.monitored_epics.keys()):&#10;                    try:&#10;                        epic_state = self.monitored_epics[epic_id]&#10;                        &#10;                        # Skip if too many consecutive errors&#10;                        if epic_state.consecutive_errors &gt;= 5:&#10;                            self.logger.warning(f&quot;Skipping EPIC {epic_id} due to consecutive errors&quot;)&#10;                            continue&#10;                        &#10;                        # Check for changes&#10;                        if self._check_epic_changes(epic_id):&#10;                            if self.config.auto_sync:&#10;                                # Schedule sync&#10;                                future = asyncio.get_event_loop().run_in_executor(&#10;                                    self.executor, self._sync_epic, epic_id&#10;                                )&#10;                                sync_tasks.append((epic_id, future))&#10;                            else:&#10;                                self.logger.info(f&quot;Changes detected in EPIC {epic_id}, but auto-sync is disabled&quot;)&#10;                        &#10;                        # Update last check time&#10;                        epic_state.last_check = datetime.now()&#10;                        &#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Error processing EPIC {epic_id}: {e}&quot;)&#10;                        import traceback&#10;                        self.logger.error(traceback.format_exc())&#10;&#10;                # Wait for sync tasks to complete&#10;                if sync_tasks:&#10;                    self.logger.info(f&quot;Running {len(sync_tasks)} synchronization tasks&quot;)&#10;                    for epic_id, future in sync_tasks:&#10;                        try:&#10;                            await future&#10;                        except Exception as e:&#10;                            self.logger.error(f&quot;Sync task failed for EPIC {epic_id}: {e}&quot;)&#10;                            import traceback&#10;                            self.logger.error(traceback.format_exc())&#10;&#10;                # Wait before next polling cycle&#10;                self.logger.debug(f&quot;Monitoring cycle complete, sleeping for {self.config.poll_interval_seconds} seconds&quot;)&#10;                await asyncio.sleep(self.config.poll_interval_seconds)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Error in monitoring loop: {e}&quot;)&#10;                import traceback&#10;                self.logger.error(traceback.format_exc())&#10;                await asyncio.sleep(60)  # Wait a minute before retrying&#10;    &#10;    def fetch_all_epic_ids(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Fetch all Epic IDs from Azure DevOps.&quot;&quot;&quot;&#10;        try:&#10;            requirements = self.agent.ado_client.get_requirements()&#10;            return [str(req.id) for req in requirements]&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to fetch all Epics: {e}&quot;)&#10;            return []&#10;&#10;    def update_monitored_epics(self):&#10;        &quot;&quot;&quot;Update the monitored Epics set by auto-detecting new Epics.&quot;&quot;&quot;&#10;        all_epic_ids = set(self.fetch_all_epic_ids())&#10;        current_epic_ids = set(self.monitored_epics.keys())&#10;        new_epics = all_epic_ids - current_epic_ids&#10;        for epic_id in new_epics:&#10;            self.logger.info(f&quot;Auto-detect: Adding new Epic {epic_id} to monitoring.&quot;)&#10;            self.add_epic(epic_id)&#10;        # Optionally, remove Epics that no longer exist in ADO&#10;        # removed_epics = current_epic_ids - all_epic_ids&#10;        # for epic_id in removed_epics:&#10;        #     self.logger.info(f&quot;Auto-detect: Removing Epic {epic_id} (no longer exists in ADO).&quot;)&#10;        #     self.monitored_epics.pop(epic_id, None)&#10;&#10;    def start(self):&#10;        &quot;&quot;&quot;Start the monitoring service&quot;&quot;&quot;&#10;        if self.is_running:&#10;            self.logger.warning(&quot;Monitor is already running&quot;)&#10;            return&#10;        &#10;        self.is_running = True&#10;        self.logger.info(&quot;Starting EPIC Change Monitor&quot;)&#10;        self.logger.info(f&quot;Monitoring {len(self.monitored_epics)} EPICs&quot;)&#10;        self.logger.info(f&quot;Poll interval: {self.config.poll_interval_seconds} seconds&quot;)&#10;        self.logger.info(f&quot;Auto-sync enabled: {self.config.auto_sync}&quot;)&#10;        &#10;        # Setup signal handlers for graceful shutdown&#10;        signal.signal(signal.SIGINT, self._signal_handler)&#10;        signal.signal(signal.SIGTERM, self._signal_handler)&#10;        &#10;        # Run the monitoring loop&#10;        try:&#10;            asyncio.run(self._monitor_loop())&#10;        except KeyboardInterrupt:&#10;            self.logger.info(&quot;Received interrupt signal&quot;)&#10;        finally:&#10;            self.stop()&#10;    &#10;    def stop(self):&#10;        &quot;&quot;&quot;Stop the monitoring service&quot;&quot;&quot;&#10;        if not self.is_running:&#10;            return&#10;        &#10;        self.logger.info(&quot;Stopping EPIC Change Monitor&quot;)&#10;        self.is_running = False&#10;        self.executor.shutdown(wait=True)&#10;        self.logger.info(&quot;EPIC Change Monitor stopped&quot;)&#10;    &#10;    def _signal_handler(self, signum, frame):&#10;        &quot;&quot;&quot;Handle shutdown signals&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Received signal {signum}, shutting down gracefully...&quot;)&#10;        self.stop()&#10;        sys.exit(0)&#10;    &#10;    def get_status(self) -&gt; Dict:&#10;        &quot;&quot;&quot;Get current monitoring status&quot;&quot;&quot;&#10;        status = {&#10;            'is_running': self.is_running,&#10;            'config': asdict(self.config),&#10;            'monitored_epics': {},&#10;            'last_update': datetime.now().isoformat()&#10;        }&#10;        &#10;        for epic_id, state in self.monitored_epics.items():&#10;            status['monitored_epics'][epic_id] = {&#10;                'last_check': state.last_check.isoformat(),&#10;                'consecutive_errors': state.consecutive_errors,&#10;                'has_snapshot': state.last_snapshot is not None,&#10;                'last_sync_result': state.last_sync_result&#10;            }&#10;        &#10;        return status&#10;    &#10;    def force_check(self, epic_id: Optional[str] = None) -&gt; Dict:&#10;        &quot;&quot;&quot;Force a check for changes (optionally for specific EPIC)&quot;&quot;&quot;&#10;        results = {}&#10;        &#10;        epics_to_check = [epic_id] if epic_id else list(self.monitored_epics.keys())&#10;        &#10;        for eid in epics_to_check:&#10;            if eid in self.monitored_epics:&#10;                try:&#10;                    has_changes = self._check_epic_changes(eid)&#10;                    results[eid] = {&#10;                        'has_changes': has_changes,&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;                    &#10;                    if has_changes and self.config.auto_sync:&#10;                        sync_result = self._sync_epic(eid)&#10;                        results[eid]['sync_result'] = {&#10;                            'success': sync_result.sync_successful,&#10;                            'created_stories': sync_result.created_stories,&#10;                            'updated_stories': sync_result.updated_stories,&#10;                            'error_message': sync_result.error_message&#10;                        }&#10;                except Exception as e:&#10;                    results[eid] = {&#10;                        'error': str(e),&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;        &#10;        return results&#10;&#10;&#10;def load_config_from_file(config_file: str) -&gt; MonitorConfig:&#10;    &quot;&quot;&quot;Load monitor configuration from JSON file&quot;&quot;&quot;&#10;    try:&#10;        with open(config_file, 'r') as f:&#10;            config_data = json.load(f)&#10;        return MonitorConfig(**config_data)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to load config from {config_file}: {e}&quot;)&#10;        return MonitorConfig()&#10;&#10;&#10;def create_default_config(config_file: str = &quot;monitor_config.json&quot;):&#10;    &quot;&quot;&quot;Create a default configuration file&quot;&quot;&quot;&#10;    default_config = MonitorConfig(&#10;        poll_interval_seconds=300,  # 5 minutes&#10;        max_concurrent_syncs=3,&#10;        snapshot_directory=&quot;snapshots&quot;,&#10;        log_level=&quot;INFO&quot;,&#10;        epic_ids=[&quot;12345&quot;, &quot;67890&quot;],  # Example EPIC IDs&#10;        auto_sync=True,&#10;        retry_attempts=3,&#10;        retry_delay_seconds=60&#10;    )&#10;    &#10;    with open(config_file, 'w') as f:&#10;        json.dump(asdict(default_config), f, indent=2)&#10;    &#10;    print(f&quot;Created default configuration file: {config_file}&quot;)&#10;    return default_config" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>