<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/agent.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/agent.py" />
              <option name="originalContent" value="import logging&#10;from typing import List, Optional, Dict, Any&#10;&#10;from src.ado_client import ADOClient&#10;from src.story_extractor import StoryExtractor&#10;from src.models import Requirement, StoryExtractionResult, UserStory, ChangeDetectionResult, EpicSyncResult&#10;&#10;class StoryExtractionAgent:&#10;    &quot;&quot;&quot;Main agent that orchestrates the story extraction process&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.ado_client = ADOClient()&#10;        self.story_extractor = StoryExtractor()&#10;        self.logger = self._setup_logger()&#10;    &#10;    def process_requirement_by_id(self, requirement_id: str, upload_to_ado: bool = True) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Process a single requirement by ID or title (string or int)&quot;&quot;&quot;&#10;        print(f&quot;\n[AGENT] Starting to process requirement ID: {requirement_id}&quot;)&#10;        try:&#10;            # Try to fetch requirement by ID or title (string or int)&#10;            print(&quot;[AGENT] Fetching requirement from Azure DevOps...&quot;)&#10;            requirement = self.ado_client.get_requirement_by_id(requirement_id)&#10;&#10;            if not requirement:&#10;                error_msg = f&quot;Requirement {requirement_id} not found or access denied&quot;&#10;                print(f&quot;[ERROR] {error_msg}&quot;)&#10;                return StoryExtractionResult(&#10;                    requirement_id=requirement_id,&#10;                    requirement_title=&quot;&quot;,&#10;                    stories=[],&#10;                    extraction_successful=False,&#10;                    error_message=error_msg&#10;                )&#10;&#10;            print(f&quot;[AGENT] Found requirement: {requirement.title}&quot;)&#10;&#10;            # Extract stories&#10;            print(&quot;[DEBUG] StoryExtractionAgent: Starting story extraction&quot;)&#10;            result = self.story_extractor.extract_stories(requirement)&#10;            &#10;            if not result.extraction_successful:&#10;                print(f&quot;[ERROR] StoryExtractionAgent: Story extraction failed: {result.error_message}&quot;)&#10;                return result&#10;            &#10;            print(f&quot;[DEBUG] StoryExtractionAgent: Successfully extracted {len(result.stories)} stories&quot;)&#10;&#10;            # Upload to ADO if requested&#10;            if upload_to_ado and result.stories:&#10;                print(&quot;[DEBUG] StoryExtractionAgent: Starting upload to ADO&quot;)&#10;                try:&#10;                    uploaded_story_ids = self._upload_stories_to_ado(result.stories, requirement_id)&#10;                    print(f&quot;[DEBUG] StoryExtractionAgent: Successfully uploaded {len(uploaded_story_ids)} stories&quot;)&#10;                except Exception as e:&#10;                    print(f&quot;[ERROR] StoryExtractionAgent: Failed to upload stories: {str(e)}&quot;)&#10;                    result.error_message = f&quot;Failed to upload stories: {str(e)}&quot;&#10;                    result.extraction_successful = False&#10;&#10;        except Exception as e:&#10;            # Accept string-based IDs (e.g., 'EPIC 1')&#10;            ado_id = requirement_id.strip()&#10;            print(f&quot;[AGENT] Using requirement ID: {ado_id}&quot;)&#10;            try:&#10;                # Get all requirements&#10;                requirement = self.ado_client.get_requirement_by_id(ado_id)&#10;                self.logger.info(f&quot;Found requirement to process: {ado_id}&quot;)&#10;                result = self.process_requirement_by_id(str(requirement.id), upload_to_ado)&#10;                # Summary&#10;                successful = 1 if result.extraction_successful else 0&#10;                total_stories = len(result.stories)&#10;                print(f&quot;[SUMMARY] Processed 1 requirement. Successful: {successful}, Total stories: {total_stories}&quot;)&#10;                return [result]&#10;            except Exception as inner_e:&#10;                print(f&quot;[ERROR] Failed to process requirement {ado_id}: {str(inner_e)}&quot;)&#10;                return []&#10;&#10;    def preview_stories(self, requirement_id: str) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Extract and preview stories without uploading to ADO&quot;&quot;&quot;&#10;        return self.process_requirement_by_id(requirement_id, upload_to_ado=False)&#10;    &#10;    def _upload_stories_to_ado(self, stories: List[UserStory], parent_requirement_id: str) -&gt; List[int]:&#10;        &quot;&quot;&quot;Upload user stories to ADO as child items of the requirement&quot;&quot;&quot;&#10;        parent_id = parent_requirement_id  # No numeric parsing anymore&#10;        uploaded_ids = []&#10;        &#10;        for story in stories:&#10;            try:&#10;                story_data = story.to_ado_format()&#10;                story_id = self.ado_client.create_user_story(story_data, parent_id)&#10;                uploaded_ids.append(story_id)&#10;                self.logger.info(f&quot;Created user story {story_id}: {story.heading}&quot;)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Failed to create user story '{story.heading}': {str(e)}&quot;)&#10;                continue&#10;        &#10;        return uploaded_ids&#10;    &#10;    def get_requirement_summary(self, requirement_id: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Get a summary of a requirement and its child stories&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = requirement_id  # No numeric parsing&#10;            requirement = self.ado_client.get_requirement_by_id(numeric_id)&#10;            if not requirement:&#10;                return {&quot;error&quot;: f&quot;Requirement {requirement_id} not found&quot;}&#10;            &#10;            child_story_ids = self.ado_client.get_child_stories(numeric_id)&#10;            &#10;            return {&#10;                &quot;requirement&quot;: {&#10;                    &quot;id&quot;: requirement.id,&#10;                    &quot;title&quot;: requirement.title,&#10;                    &quot;description&quot;: requirement.description[:200] + &quot;...&quot; if len(requirement.description) &gt; 200 else requirement.description,&#10;                    &quot;state&quot;: requirement.state&#10;                },&#10;                &quot;child_stories&quot;: {&#10;                    &quot;count&quot;: len(child_story_ids),&#10;                    &quot;ids&quot;: child_story_ids&#10;                }&#10;            }&#10;            &#10;        except Exception as e:&#10;            return {&quot;error&quot;: str(e)}&#10;    &#10;    def synchronize_epic(self, epic_id: str, stored_snapshot: Optional[Dict[str, str]] = None) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Detect changes in an EPIC and synchronize its tasks&quot;&quot;&quot;&#10;        try:&#10;            self.logger.info(f&quot;Synchronizing EPIC {epic_id}&quot;)&#10;&#10;            numeric_epic_id = epic_id  # No numeric parsing&#10;&#10;            # Get current EPIC state&#10;            current_epic = self.ado_client.get_requirement_by_id(epic_id)&#10;            if not current_epic:&#10;                raise Exception(f&quot;EPIC {epic_id} not found&quot;)&#10;&#10;            # Get current snapshot for change detection&#10;            current_snapshot = self.ado_client.detect_changes_in_epic(numeric_epic_id)&#10;            existing_stories = self.ado_client.get_existing_user_stories(numeric_epic_id)&#10;&#10;            self.logger.info(f&quot;Found {len(existing_stories)} existing user stories for EPIC {epic_id}&quot;)&#10;&#10;            # Check if changes detected (compare with stored snapshot if provided)&#10;            has_changes = True  # Always extract stories for now, can be refined later&#10;            changes_detected = []&#10;&#10;            if stored_snapshot and current_snapshot:&#10;                stored_hash = stored_snapshot.get('content_hash', '')&#10;                if stored_hash == current_snapshot.content_hash:&#10;                    has_changes = False&#10;                    self.logger.info(f&quot;No content changes detected for EPIC {epic_id}&quot;)&#10;                else:&#10;                    changes_detected.append(&quot;EPIC content has been modified&quot;)&#10;                    self.logger.info(f&quot;Content changes detected for EPIC {epic_id}&quot;)&#10;            else:&#10;                changes_detected.append(&quot;Initial sync or no previous snapshot available&quot;)&#10;&#10;            sync_result = EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=current_epic.title&#10;            )&#10;&#10;            if has_changes:&#10;                # Extract new stories from the updated EPIC&#10;                story_extraction_result = self.story_extractor.extract_stories(current_epic)&#10;&#10;                if not story_extraction_result.extraction_successful:&#10;                    raise Exception(f&quot;Story extraction failed: {story_extraction_result.error_message}&quot;)&#10;&#10;                new_stories = story_extraction_result.stories&#10;                self.logger.info(f&quot;Extracted {len(new_stories)} stories from updated EPIC&quot;)&#10;&#10;                # Determine which stories to create, update, or leave unchanged&#10;                stories_to_create, stories_to_update, unchanged_stories = self._analyze_story_changes(&#10;                    existing_stories, new_stories&#10;                )&#10;&#10;                self.logger.info(f&quot;Analysis: {len(stories_to_create)} to create, {len(stories_to_update)} to update, {len(unchanged_stories)} unchanged&quot;)&#10;&#10;                # Create new stories&#10;                created_ids = []&#10;                for story in stories_to_create:&#10;                    try:&#10;                        story_data = story.to_ado_format()&#10;                        story_id = self.ado_client.create_user_story(story_data, numeric_epic_id)&#10;                        created_ids.append(story_id)&#10;                        self.logger.info(f&quot;Created new user story {story_id}: {story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to create story '{story.heading}': {str(e)}&quot;)&#10;                        continue&#10;&#10;                # Update existing stories&#10;                updated_ids = []&#10;                for story_update in stories_to_update:&#10;                    try:&#10;                        story_id = story_update['id']&#10;                        new_story = story_update['new_story']&#10;                        self._update_user_story(story_id, new_story)&#10;                        updated_ids.append(story_id)&#10;                        self.logger.info(f&quot;Updated user story {story_id}: {new_story.heading}&quot;)&#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Failed to update story {story_update['id']}: {str(e)}&quot;)&#10;                        continue&#10;&#10;                sync_result.created_stories = created_ids&#10;                sync_result.updated_stories = updated_ids&#10;                sync_result.unchanged_stories = [s.id for s in unchanged_stories]&#10;&#10;                self.logger.info(f&quot;EPIC sync completed: {len(created_ids)} created, {len(updated_ids)} updated, {len(unchanged_stories)} unchanged&quot;)&#10;            else:&#10;                sync_result.unchanged_stories = [s.id for s in existing_stories]&#10;                self.logger.info(f&quot;No changes detected, {len(existing_stories)} stories remain unchanged&quot;)&#10;&#10;            return sync_result&#10;&#10;        except Exception as e:&#10;            self.logger.error(f&quot;EPIC synchronization failed for {epic_id}: {str(e)}&quot;)&#10;            return EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=&quot;&quot;,&#10;                sync_successful=False,&#10;                error_message=str(e)&#10;            )&#10;    &#10;    def _analyze_story_changes(self, existing_stories, new_stories):&#10;        &quot;&quot;&quot;Analyze differences between existing and new stories to determine what to create/update&quot;&quot;&quot;&#10;        from difflib import SequenceMatcher&#10;        &#10;        stories_to_create = []&#10;        stories_to_update = []&#10;        unchanged_stories = []&#10;        &#10;        # Convert existing stories to a dict for easier lookup&#10;        existing_by_title = {story.title: story for story in existing_stories}&#10;        &#10;        # Check each new story against existing ones&#10;        for new_story in new_stories:&#10;            best_match = None&#10;            best_similarity = 0.0&#10;            &#10;            # Find the best matching existing story by title similarity&#10;            for existing_title, existing_story in existing_by_title.items():&#10;                similarity = SequenceMatcher(None, new_story.heading.lower(), existing_title.lower()).ratio()&#10;                if similarity &gt; best_similarity:&#10;                    best_similarity = similarity&#10;                    best_match = existing_story&#10;            &#10;            # If we found a good match (similarity &gt; 0.8), consider it for update&#10;            if best_match and best_similarity &gt; 0.8:&#10;                # Check if the content has actually changed&#10;                existing_content = f&quot;{best_match.title} {best_match.description}&quot;&#10;                new_content = f&quot;{new_story.heading} {new_story.description} {' '.join(new_story.acceptance_criteria)}&quot;&#10;                &#10;                content_similarity = SequenceMatcher(None, existing_content.lower(), new_content.lower()).ratio()&#10;                &#10;                if content_similarity &lt; 0.9:  # Content has changed significantly&#10;                    stories_to_update.append({&#10;                        'id': best_match.id,&#10;                        'existing_story': best_match,&#10;                        'new_story': new_story&#10;                    })&#10;                    # Remove from existing dict so it's not considered again&#10;                    del existing_by_title[best_match.title]&#10;                else:&#10;                    unchanged_stories.append(best_match)&#10;                    del existing_by_title[best_match.title]&#10;            else:&#10;                # No good match found, this is a new story&#10;                stories_to_create.append(new_story)&#10;        &#10;        # Any remaining existing stories that weren't matched are considered unchanged&#10;        for remaining_story in existing_by_title.values():&#10;            unchanged_stories.append(remaining_story)&#10;        &#10;        return stories_to_create, stories_to_update, unchanged_stories&#10;    &#10;    def _update_user_story(self, story_id: int, new_story: UserStory):&#10;        &quot;&quot;&quot;Update an existing user story in ADO&quot;&quot;&quot;&#10;        try:&#10;            story_data = new_story.to_ado_format()&#10;            &#10;            # Prepare update document&#10;            document = []&#10;            for field, value in story_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.ado_client.wit_client.update_work_item(&#10;                document=document,&#10;                id=story_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update user story {story_id}: {str(e)}&quot;)&#10;    &#10;    def get_epic_snapshot(self, epic_id: str) -&gt; Optional[Dict[str, str]]:&#10;        &quot;&quot;&quot;Get a snapshot of the current EPIC for change tracking&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = epic_id  # No numeric parsing&#10;            snapshot = self.ado_client.detect_changes_in_epic(numeric_id)&#10;            &#10;            if snapshot:&#10;                return {&#10;                    'content_hash': snapshot.content_hash,&#10;                    'last_modified': snapshot.last_modified.isoformat() if snapshot.last_modified else None,&#10;                    'title': snapshot.title,&#10;                    'state': snapshot.state&#10;                }&#10;            return None&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get EPIC snapshot for {epic_id}: {str(e)}&quot;)&#10;            return None&#10;&#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging configuration&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;StoryExtractionAgent&quot;)&#10;        logger.setLevel(logging.INFO)&#10;        &#10;        if not logger.handlers:&#10;            handler = logging.StreamHandler()&#10;            formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            handler.setFormatter(formatter)&#10;            logger.addHandler(handler)&#10;        &#10;        return logger&#10;" />
              <option name="updatedContent" value="import logging&#10;from typing import List, Optional, Dict, Any&#10;&#10;from src.ado_client import ADOClient&#10;from src.story_extractor import StoryExtractor&#10;from src.models import Requirement, StoryExtractionResult, UserStory, ChangeDetectionResult, EpicSyncResult&#10;&#10;class StoryExtractionAgent:&#10;    &quot;&quot;&quot;Main agent that orchestrates the story extraction process&quot;&quot;&quot;&#10;    &#10;    def __init__(self):&#10;        self.ado_client = ADOClient()&#10;        self.story_extractor = StoryExtractor()&#10;        self.logger = self._setup_logger()&#10;    &#10;    def process_requirement_by_id(self, requirement_id: str, upload_to_ado: bool = True) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Process a single requirement by ID or title (string or int)&quot;&quot;&quot;&#10;        print(f&quot;\n[AGENT] Starting to process requirement ID: {requirement_id}&quot;)&#10;        try:&#10;            # Try to fetch requirement by ID or title (string or int)&#10;            print(&quot;[AGENT] Fetching requirement from Azure DevOps...&quot;)&#10;            requirement = self.ado_client.get_requirement_by_id(requirement_id)&#10;&#10;            if not requirement:&#10;                error_msg = f&quot;Requirement {requirement_id} not found or access denied&quot;&#10;                print(f&quot;[ERROR] {error_msg}&quot;)&#10;                return StoryExtractionResult(&#10;                    requirement_id=requirement_id,&#10;                    requirement_title=&quot;&quot;,&#10;                    stories=[],&#10;                    extraction_successful=False,&#10;                    error_message=error_msg&#10;                )&#10;&#10;            print(f&quot;[AGENT] Found requirement: {requirement.title}&quot;)&#10;&#10;            # Extract stories&#10;            print(&quot;[DEBUG] StoryExtractionAgent: Starting story extraction&quot;)&#10;            result = self.story_extractor.extract_stories(requirement)&#10;            &#10;            if not result.extraction_successful:&#10;                print(f&quot;[ERROR] StoryExtractionAgent: Story extraction failed: {result.error_message}&quot;)&#10;                return result&#10;            &#10;            print(f&quot;[DEBUG] StoryExtractionAgent: Successfully extracted {len(result.stories)} stories&quot;)&#10;&#10;            # Upload to ADO if requested&#10;            if upload_to_ado and result.stories:&#10;                print(&quot;[DEBUG] StoryExtractionAgent: Starting upload to ADO&quot;)&#10;                try:&#10;                    uploaded_story_ids = self._upload_stories_to_ado(result.stories, requirement_id)&#10;                    print(f&quot;[DEBUG] StoryExtractionAgent: Successfully uploaded {len(uploaded_story_ids)} stories&quot;)&#10;                except Exception as e:&#10;                    print(f&quot;[ERROR] StoryExtractionAgent: Failed to upload stories: {str(e)}&quot;)&#10;                    result.error_message = f&quot;Failed to upload stories: {str(e)}&quot;&#10;                    result.extraction_successful = False&#10;&#10;        except Exception as e:&#10;            # Accept string-based IDs (e.g., 'EPIC 1')&#10;            ado_id = requirement_id.strip()&#10;            print(f&quot;[AGENT] Using requirement ID: {ado_id}&quot;)&#10;            try:&#10;                # Get all requirements&#10;                requirement = self.ado_client.get_requirement_by_id(ado_id)&#10;                self.logger.info(f&quot;Found requirement to process: {ado_id}&quot;)&#10;                result = self.process_requirement_by_id(str(requirement.id), upload_to_ado)&#10;                # Summary&#10;                successful = 1 if result.extraction_successful else 0&#10;                total_stories = len(result.stories)&#10;                print(f&quot;[SUMMARY] Processed 1 requirement. Successful: {successful}, Total stories: {total_stories}&quot;)&#10;                return [result]&#10;            except Exception as inner_e:&#10;                print(f&quot;[ERROR] Failed to process requirement {ado_id}: {str(inner_e)}&quot;)&#10;                return []&#10;&#10;    def preview_stories(self, requirement_id: str) -&gt; StoryExtractionResult:&#10;        &quot;&quot;&quot;Extract and preview stories without uploading to ADO&quot;&quot;&quot;&#10;        return self.process_requirement_by_id(requirement_id, upload_to_ado=False)&#10;    &#10;    def _upload_stories_to_ado(self, stories: List[UserStory], parent_requirement_id: str) -&gt; List[int]:&#10;        &quot;&quot;&quot;Upload user stories to ADO as child items of the requirement&quot;&quot;&quot;&#10;        parent_id = parent_requirement_id  # No numeric parsing anymore&#10;        uploaded_ids = []&#10;        &#10;        for story in stories:&#10;            try:&#10;                story_data = story.to_ado_format()&#10;                story_id = self.ado_client.create_user_story(story_data, parent_id)&#10;                uploaded_ids.append(story_id)&#10;                self.logger.info(f&quot;Created user story {story_id}: {story.heading}&quot;)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Failed to create user story '{story.heading}': {str(e)}&quot;)&#10;                continue&#10;        &#10;        return uploaded_ids&#10;    &#10;    def get_requirement_summary(self, requirement_id: str) -&gt; Dict[str, Any]:&#10;        &quot;&quot;&quot;Get a summary of a requirement and its child stories&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = requirement_id  # No numeric parsing&#10;            requirement = self.ado_client.get_requirement_by_id(numeric_id)&#10;            if not requirement:&#10;                return {&quot;error&quot;: f&quot;Requirement {requirement_id} not found&quot;}&#10;            &#10;            child_story_ids = self.ado_client.get_child_stories(numeric_id)&#10;            &#10;            return {&#10;                &quot;requirement&quot;: {&#10;                    &quot;id&quot;: requirement.id,&#10;                    &quot;title&quot;: requirement.title,&#10;                    &quot;description&quot;: requirement.description[:200] + &quot;...&quot; if len(requirement.description) &gt; 200 else requirement.description,&#10;                    &quot;state&quot;: requirement.state&#10;                },&#10;                &quot;child_stories&quot;: {&#10;                    &quot;count&quot;: len(child_story_ids),&#10;                    &quot;ids&quot;: child_story_ids&#10;                }&#10;            }&#10;            &#10;        except Exception as e:&#10;            return {&quot;error&quot;: str(e)}&#10;    &#10;    def synchronize_epic(self, epic_id: str, stored_snapshot: Optional[Dict] = None) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Detect changes in an EPIC and synchronize its tasks&quot;&quot;&quot;&#10;        self.logger.info(f&quot;[AGENT] Synchronizing Epic: {epic_id}&quot;)&#10;        try:&#10;            # Fetch the requirement (Epic) from ADO&#10;            requirement = self.ado_client.get_requirement_by_id(epic_id)&#10;            if not requirement:&#10;                error_msg = f&quot;[AGENT] Epic {epic_id} not found or access denied&quot;&#10;                self.logger.error(error_msg)&#10;                return EpicSyncResult(&#10;                    epic_id=epic_id,&#10;                    epic_title=&quot;&quot;,&#10;                    sync_successful=False,&#10;                    error_message=error_msg&#10;                )&#10;            self.logger.info(f&quot;[AGENT] Fetched Epic: {requirement.title}&quot;)&#10;            self.logger.info(f&quot;[AGENT] Epic Description: {requirement.description}&quot;)&#10;            # Extract stories&#10;            self.logger.info(f&quot;[AGENT] Extracting stories from Epic {epic_id}&quot;)&#10;            extraction_result = self.story_extractor.extract_stories(requirement)&#10;            if not extraction_result.extraction_successful:&#10;                self.logger.error(f&quot;[AGENT] Story extraction failed: {extraction_result.error_message}&quot;)&#10;                return EpicSyncResult(&#10;                    epic_id=epic_id,&#10;                    epic_title=requirement.title,&#10;                    sync_successful=False,&#10;                    error_message=extraction_result.error_message&#10;                )&#10;            self.logger.info(f&quot;[AGENT] Extracted {len(extraction_result.stories)} stories from Epic {epic_id}&quot;)&#10;            # Upload stories to ADO&#10;            created_stories = []&#10;            updated_stories = []&#10;            unchanged_stories = []&#10;            if extraction_result.stories:&#10;                self.logger.info(f&quot;[AGENT] Uploading {len(extraction_result.stories)} stories to ADO for Epic {epic_id}&quot;)&#10;                for story in extraction_result.stories:&#10;                    # TODO: Replace the following with actual upload logic that returns the new story's integer ID&#10;                    # Example: story_id = self.ado_client.create_story(story)&#10;                    story_id = None  # Placeholder for the created story's ID&#10;                    try:&#10;                        # Replace with actual upload logic and get the ID&#10;                        story_id = self.ado_client.create_story(story)  # This should return an int ID&#10;                    except Exception as upload_exc:&#10;                        self.logger.error(f&quot;[AGENT] Failed to upload story '{story.heading}': {upload_exc}&quot;)&#10;                        continue&#10;                    if isinstance(story_id, int):&#10;                        created_stories.append(story_id)&#10;                    else:&#10;                        self.logger.error(f&quot;[AGENT] Story upload did not return a valid integer ID for '{story.heading}'&quot;)&#10;            else:&#10;                self.logger.info(f&quot;[AGENT] No stories extracted for Epic {epic_id}&quot;)&#10;            return EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=requirement.title,&#10;                sync_successful=True,&#10;                created_stories=created_stories,&#10;                updated_stories=updated_stories,&#10;                unchanged_stories=unchanged_stories&#10;            )&#10;        except Exception as e:&#10;            self.logger.error(f&quot;[AGENT] Exception during Epic sync: {e}&quot;)&#10;            import traceback&#10;            self.logger.error(traceback.format_exc())&#10;            return EpicSyncResult(&#10;                epic_id=epic_id,&#10;                epic_title=&quot;&quot;,&#10;                sync_successful=False,&#10;                error_message=str(e)&#10;            )&#10;    &#10;    def _analyze_story_changes(self, existing_stories, new_stories):&#10;        &quot;&quot;&quot;Analyze differences between existing and new stories to determine what to create/update&quot;&quot;&quot;&#10;        from difflib import SequenceMatcher&#10;        &#10;        stories_to_create = []&#10;        stories_to_update = []&#10;        unchanged_stories = []&#10;        &#10;        # Convert existing stories to a dict for easier lookup&#10;        existing_by_title = {story.title: story for story in existing_stories}&#10;        &#10;        # Check each new story against existing ones&#10;        for new_story in new_stories:&#10;            best_match = None&#10;            best_similarity = 0.0&#10;            &#10;            # Find the best matching existing story by title similarity&#10;            for existing_title, existing_story in existing_by_title.items():&#10;                similarity = SequenceMatcher(None, new_story.heading.lower(), existing_title.lower()).ratio()&#10;                if similarity &gt; best_similarity:&#10;                    best_similarity = similarity&#10;                    best_match = existing_story&#10;            &#10;            # If we found a good match (similarity &gt; 0.8), consider it for update&#10;            if best_match and best_similarity &gt; 0.8:&#10;                # Check if the content has actually changed&#10;                existing_content = f&quot;{best_match.title} {best_match.description}&quot;&#10;                new_content = f&quot;{new_story.heading} {new_story.description} {' '.join(new_story.acceptance_criteria)}&quot;&#10;                &#10;                content_similarity = SequenceMatcher(None, existing_content.lower(), new_content.lower()).ratio()&#10;                &#10;                if content_similarity &lt; 0.9:  # Content has changed significantly&#10;                    stories_to_update.append({&#10;                        'id': best_match.id,&#10;                        'existing_story': best_match,&#10;                        'new_story': new_story&#10;                    })&#10;                    # Remove from existing dict so it's not considered again&#10;                    del existing_by_title[best_match.title]&#10;                else:&#10;                    unchanged_stories.append(best_match)&#10;                    del existing_by_title[best_match.title]&#10;            else:&#10;                # No good match found, this is a new story&#10;                stories_to_create.append(new_story)&#10;        &#10;        # Any remaining existing stories that weren't matched are considered unchanged&#10;        for remaining_story in existing_by_title.values():&#10;            unchanged_stories.append(remaining_story)&#10;        &#10;        return stories_to_create, stories_to_update, unchanged_stories&#10;    &#10;    def _update_user_story(self, story_id: int, new_story: UserStory):&#10;        &quot;&quot;&quot;Update an existing user story in ADO&quot;&quot;&quot;&#10;        try:&#10;            story_data = new_story.to_ado_format()&#10;            &#10;            # Prepare update document&#10;            document = []&#10;            for field, value in story_data.items():&#10;                document.append({&#10;                    &quot;op&quot;: &quot;replace&quot;,&#10;                    &quot;path&quot;: f&quot;/fields/{field}&quot;,&#10;                    &quot;value&quot;: value&#10;                })&#10;            &#10;            # Update the work item&#10;            self.ado_client.wit_client.update_work_item(&#10;                document=document,&#10;                id=story_id&#10;            )&#10;            &#10;        except Exception as e:&#10;            raise Exception(f&quot;Failed to update user story {story_id}: {str(e)}&quot;)&#10;    &#10;    def get_epic_snapshot(self, epic_id: str) -&gt; Optional[Dict[str, str]]:&#10;        &quot;&quot;&quot;Get a snapshot of the current EPIC for change tracking&quot;&quot;&quot;&#10;        try:&#10;            numeric_id = epic_id  # No numeric parsing&#10;            snapshot = self.ado_client.detect_changes_in_epic(numeric_id)&#10;            &#10;            if snapshot:&#10;                return {&#10;                    'content_hash': snapshot.content_hash,&#10;                    'last_modified': snapshot.last_modified.isoformat() if snapshot.last_modified else None,&#10;                    'title': snapshot.title,&#10;                    'state': snapshot.state&#10;                }&#10;            return None&#10;            &#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to get EPIC snapshot for {epic_id}: {str(e)}&quot;)&#10;            return None&#10;&#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging configuration&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;StoryExtractionAgent&quot;)&#10;        logger.setLevel(logging.INFO)&#10;        &#10;        if not logger.handlers:&#10;            handler = logging.StreamHandler()&#10;            formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            handler.setFormatter(formatter)&#10;            logger.addHandler(handler)&#10;        &#10;        return logger" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/monitor.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/monitor.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;Background monitoring service for EPIC change detection and automatic synchronization.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import json&#10;import logging&#10;import os&#10;import signal&#10;import sys&#10;import time&#10;from datetime import datetime, timedelta&#10;from pathlib import Path&#10;from typing import Dict, List, Optional, Set, ClassVar&#10;from dataclasses import dataclass, asdict&#10;from concurrent.futures import ThreadPoolExecutor&#10;&#10;from src.agent import StoryExtractionAgent&#10;from src.models import EpicSyncResult&#10;&#10;&#10;@dataclass&#10;class MonitorConfig:&#10;    &quot;&quot;&quot;Configuration for the EPIC monitor&quot;&quot;&quot;&#10;    OPENAI_RETRY_DELAY: ClassVar[int] = int(os.getenv('OPENAI_RETRY_DELAY', 5))&#10;    poll_interval_seconds: int = 300  # 5 minutes default&#10;    max_concurrent_syncs: int = 3&#10;    snapshot_directory: str = &quot;snapshots&quot;&#10;    log_level: str = &quot;INFO&quot;&#10;    epic_ids: List[str] = None&#10;    auto_sync: bool = True&#10;    notification_webhook: Optional[str] = None&#10;    retry_attempts: int = 3&#10;    retry_delay_seconds: int = 60&#10;&#10;&#10;@dataclass&#10;class EpicMonitorState:&#10;    &quot;&quot;&quot;State tracking for a monitored EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    last_check: datetime&#10;    last_snapshot: Optional[Dict] = None&#10;    consecutive_errors: int = 0&#10;    last_sync_result: Optional[Dict] = None&#10;&#10;&#10;class EpicChangeMonitor:&#10;    &quot;&quot;&quot;Background service that monitors EPICs for changes and triggers synchronization&quot;&quot;&quot;&#10;    &#10;    def __init__(self, config: MonitorConfig):&#10;        self.config = config&#10;        self.agent = StoryExtractionAgent()&#10;        self.logger = self._setup_logger()&#10;        self.is_running = False&#10;        self.monitored_epics: Dict[str, EpicMonitorState] = {}&#10;        self.executor = ThreadPoolExecutor(max_workers=config.max_concurrent_syncs)&#10;        self.snapshot_dir = Path(config.snapshot_directory)&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;        # ThreadPoolExecutor for async syncs&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;&#10;        # Load existing snapshots&#10;        self._load_existing_snapshots()&#10;    &#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging for the monitor&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;EpicChangeMonitor&quot;)&#10;        logger.setLevel(getattr(logging, self.config.log_level.upper()))&#10;        if not logger.handlers:&#10;            # Console handler&#10;            console_handler = logging.StreamHandler()&#10;            console_formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            console_handler.setFormatter(console_formatter)&#10;            logger.addHandler(console_handler)&#10;            # File handler&#10;            log_file = Path(&quot;logs&quot;) / &quot;epic_monitor.log&quot;&#10;            log_file.parent.mkdir(exist_ok=True)&#10;            file_handler = logging.FileHandler(log_file)&#10;            file_handler.setFormatter(console_formatter)&#10;            logger.addHandler(file_handler)&#10;        return logger&#10;&#10;    def _load_existing_snapshots(self):&#10;        return logger&#10;        for epic_id in self.config.epic_ids or []:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            if snapshot_file.exists():&#10;                try:&#10;                    with open(snapshot_file, 'r') as f:&#10;                        snapshot_data = json.load(f)&#10;                    &#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=snapshot_data&#10;                    )&#10;                    self.logger.info(f&quot;Loaded existing snapshot for EPIC {epic_id}&quot;)&#10;                except Exception as e:&#10;                    self.logger.error(f&quot;Failed to load snapshot for EPIC {epic_id}: {e}&quot;)&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now()&#10;                    )&#10;            else:&#10;                self.monitored_epics[epic_id] = EpicMonitorState(&#10;                    epic_id=epic_id,&#10;                    last_check=datetime.now()&#10;                )&#10;    &#10;    def add_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Add an EPIC to monitoring and trigger immediate check/sync.&quot;&quot;&quot;&#10;        try:&#10;            if epic_id not in self.monitored_epics:&#10;                # Get initial snapshot&#10;                initial_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                if initial_snapshot:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=initial_snapshot,&#10;                        consecutive_errors=0&#10;                    )&#10;                    self._save_snapshot(epic_id, initial_snapshot)&#10;                    self.logger.info(f&quot;Added EPIC {epic_id} to monitoring and will check for changes immediately.&quot;)&#10;                    # Immediately check and sync the new Epic&#10;                    if self._check_epic_changes(epic_id):&#10;                        if self.config.auto_sync:&#10;                            self.logger.info(f&quot;Immediately synchronizing new EPIC {epic_id} after detection.&quot;)&#10;                            self._sync_epic(epic_id)&#10;                    return True&#10;                else:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=None,&#10;                        consecutive_errors=1&#10;                    )&#10;                    self.logger.warning(f&quot;Added EPIC {epic_id} to monitoring, but could not fetch initial snapshot. Will retry.&quot;)&#10;                    return False&#10;            else:&#10;                self.logger.warning(f&quot;EPIC {epic_id} is already being monitored&quot;)&#10;                return True&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to add EPIC {epic_id} to monitoring: {e}&quot;)&#10;            return False&#10;    &#10;    def remove_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Remove an EPIC from monitoring&quot;&quot;&quot;&#10;        if epic_id in self.monitored_epics:&#10;            del self.monitored_epics[epic_id]&#10;            self.logger.info(f&quot;Removed EPIC {epic_id} from monitoring&quot;)&#10;            return True&#10;        return False&#10;    &#10;    def _save_snapshot(self, epic_id: str, snapshot: Dict):&#10;        &quot;&quot;&quot;Save snapshot to file&quot;&quot;&quot;&#10;        try:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            with open(snapshot_file, 'w') as f:&#10;                json.dump(snapshot, f, indent=2)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save snapshot for EPIC {epic_id}: {e}&quot;)&#10;    &#10;    def _check_epic_changes(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Check if an EPIC has changes. Remove from monitoring if undetectable for 3 retries.&quot;&quot;&quot;&#10;        try:&#10;            epic_state = self.monitored_epics[epic_id]&#10;            current_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;            &#10;            if not current_snapshot:&#10;                epic_state.consecutive_errors += 1&#10;                self.logger.warning(f&quot;Failed to get current snapshot for EPIC {epic_id} (consecutive errors: {epic_state.consecutive_errors})&quot;)&#10;                if epic_state.consecutive_errors &gt;= 3:&#10;                    self.logger.error(f&quot;EPIC {epic_id} could not be detected after 3 retries. Removing from monitoring.&quot;)&#10;                    self.remove_epic(epic_id)&#10;                return False&#10;            &#10;            # Reset error counter on successful snapshot&#10;            epic_state.consecutive_errors = 0&#10;            &#10;            # Compare with last known snapshot&#10;            if epic_state.last_snapshot:&#10;                last_hash = epic_state.last_snapshot.get('content_hash', '')&#10;                current_hash = current_snapshot.get('content_hash', '')&#10;                &#10;                if last_hash != current_hash:&#10;                    self.logger.info(f&quot;Changes detected in EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Previous hash: {last_hash[:16]}...&quot;)&#10;                    self.logger.info(f&quot;  Current hash:  {current_hash[:16]}...&quot;)&#10;                    # Update snapshot for next check&#10;                    epic_state.last_snapshot = current_snapshot&#10;                    self._save_snapshot(epic_id, current_snapshot)&#10;                    return True&#10;                else:&#10;                    self.logger.info(f&quot;No changes detected in EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                # First check, save current snapshot&#10;                self.logger.info(f&quot;Initial snapshot saved for EPIC {epic_id}&quot;)&#10;                epic_state.last_snapshot = current_snapshot&#10;                self._save_snapshot(epic_id, current_snapshot)&#10;                return True  # Treat as change to trigger sync for new Epics&#10;&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Error checking changes for EPIC {epic_id}: {e}&quot;)&#10;            if epic_id in self.monitored_epics:&#10;                self.monitored_epics[epic_id].consecutive_errors += 1&#10;                if self.monitored_epics[epic_id].consecutive_errors &gt;= 3:&#10;                    self.logger.error(f&quot;EPIC {epic_id} could not be detected after 3 retries. Removing from monitoring.&quot;)&#10;                    self.remove_epic(epic_id)&#10;            return False&#10;    &#10;    def _sync_epic(self, epic_id: str) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Synchronize an EPIC with retry logic&quot;&quot;&quot;&#10;        epic_state = self.monitored_epics[epic_id]&#10;        &#10;        for attempt in range(self.config.retry_attempts):&#10;            try:&#10;                self.logger.info(f&quot;Synchronizing EPIC {epic_id} (attempt {attempt + 1})&quot;)&#10;                &#10;                result = self.agent.synchronize_epic(&#10;                    epic_id=epic_id,&#10;                    stored_snapshot=epic_state.last_snapshot&#10;                )&#10;                &#10;                if result.sync_successful:&#10;                    # Update snapshot after successful sync&#10;                    new_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                    if new_snapshot:&#10;                        epic_state.last_snapshot = new_snapshot&#10;                        self._save_snapshot(epic_id, new_snapshot)&#10;                    &#10;                    # Store sync result&#10;                    epic_state.last_sync_result = {&#10;                        'timestamp': datetime.now().isoformat(),&#10;                        'success': True,&#10;                        'created_stories': result.created_stories,&#10;                        'updated_stories': result.updated_stories,&#10;                        'unchanged_stories': result.unchanged_stories&#10;                    }&#10;                    &#10;                    self.logger.info(f&quot;Successfully synchronized EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Created: {len(result.created_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Updated: {len(result.updated_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Unchanged: {len(result.unchanged_stories)} stories&quot;)&#10;                    &#10;                    return result&#10;                else:&#10;                    self.logger.error(f&quot;Sync failed for EPIC {epic_id}: {result.error_message}&quot;)&#10;                    if attempt &lt; self.config.retry_attempts - 1:&#10;                        self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                        time.sleep(self.config.retry_delay_seconds)&#10;                    &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Exception during sync of EPIC {epic_id}: {e}&quot;)&#10;                if attempt &lt; self.config.retry_attempts - 1:&#10;                    self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                    time.sleep(self.config.retry_delay_seconds)&#10;        &#10;        # All attempts failed&#10;        epic_state.last_sync_result = {&#10;            'timestamp': datetime.now().isoformat(),&#10;            'success': False,&#10;            'error': f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        }&#10;        &#10;        return EpicSyncResult(&#10;            epic_id=epic_id,&#10;            epic_title=&quot;&quot;,&#10;            sync_successful=False,&#10;            error_message=f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        )&#10;    &#10;    async def _monitor_loop(self):&#10;        &quot;&quot;&quot;Main monitoring loop&quot;&quot;&quot;&#10;        self.logger.info(&quot;Starting EPIC monitoring loop&quot;)&#10;        &#10;        while self.is_running:&#10;            try:&#10;                # Auto-detect new Epics at the start of each cycle&#10;                self.update_monitored_epics()&#10;                # Check each monitored EPIC&#10;                sync_tasks = []&#10;                &#10;                for epic_id in list(self.monitored_epics.keys()):&#10;                    try:&#10;                        epic_state = self.monitored_epics[epic_id]&#10;                        &#10;                        # Skip if too many consecutive errors&#10;                        if epic_state.consecutive_errors &gt;= 5:&#10;                            self.logger.warning(f&quot;Skipping EPIC {epic_id} due to consecutive errors&quot;)&#10;                            continue&#10;                        &#10;                        # Check for changes&#10;                        if self._check_epic_changes(epic_id):&#10;                            if self.config.auto_sync:&#10;                                # Schedule sync&#10;                                future = asyncio.get_event_loop().run_in_executor(&#10;                                    self.executor, self._sync_epic, epic_id&#10;                                )&#10;                                sync_tasks.append((epic_id, future))&#10;                            else:&#10;                                self.logger.info(f&quot;Changes detected in EPIC {epic_id}, but auto-sync is disabled&quot;)&#10;                        &#10;                        # Update last check time&#10;                        epic_state.last_check = datetime.now()&#10;                        &#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Error processing EPIC {epic_id}: {e}&quot;)&#10;                        import traceback&#10;                        self.logger.error(traceback.format_exc())&#10;&#10;                # Wait for sync tasks to complete&#10;                if sync_tasks:&#10;                    self.logger.info(f&quot;Running {len(sync_tasks)} synchronization tasks&quot;)&#10;                    for epic_id, future in sync_tasks:&#10;                        try:&#10;                            await future&#10;                        except Exception as e:&#10;                            self.logger.error(f&quot;Sync task failed for EPIC {epic_id}: {e}&quot;)&#10;                            import traceback&#10;                            self.logger.error(traceback.format_exc())&#10;&#10;                # Wait before next polling cycle&#10;                self.logger.debug(f&quot;Monitoring cycle complete, sleeping for {self.config.poll_interval_seconds} seconds&quot;)&#10;                await asyncio.sleep(self.config.poll_interval_seconds)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Error in monitoring loop: {e}&quot;)&#10;                import traceback&#10;                self.logger.error(traceback.format_exc())&#10;                await asyncio.sleep(60)  # Wait a minute before retrying&#10;    &#10;    def fetch_all_epic_ids(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Fetch all Epic IDs from Azure DevOps (filtered by work item type 'Epic').&quot;&quot;&quot;&#10;        try:&#10;            requirements = self.agent.ado_client.get_requirements(work_item_type=&quot;Epic&quot;)&#10;            return [str(req.id) for req in requirements]&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to fetch all Epics: {e}&quot;)&#10;            return []&#10;&#10;    def update_monitored_epics(self):&#10;        &quot;&quot;&quot;Update the monitored Epics set by auto-detecting new Epics.&quot;&quot;&quot;&#10;        all_epic_ids = set(self.fetch_all_epic_ids())&#10;        current_epic_ids = set(self.monitored_epics.keys())&#10;        new_epics = all_epic_ids - current_epic_ids&#10;        for epic_id in new_epics:&#10;            self.logger.info(f&quot;Auto-detect: Adding new Epic {epic_id} to monitoring.&quot;)&#10;            self.add_epic(epic_id)&#10;        # Optionally, remove Epics that no longer exist in ADO&#10;        # removed_epics = current_epic_ids - all_epic_ids&#10;        # for epic_id in removed_epics:&#10;        #     self.logger.info(f&quot;Auto-detect: Removing Epic {epic_id} (no longer exists in ADO).&quot;)&#10;        #     self.monitored_epics.pop(epic_id, None)&#10;&#10;    def start(self):&#10;        &quot;&quot;&quot;Start the monitoring service&quot;&quot;&quot;&#10;        if self.is_running:&#10;            self.logger.warning(&quot;Monitor is already running&quot;)&#10;            return&#10;        &#10;        self.is_running = True&#10;        self.logger.info(&quot;Starting EPIC Change Monitor&quot;)&#10;        self.logger.info(f&quot;Monitoring {len(self.monitored_epics)} EPICs&quot;)&#10;        self.logger.info(f&quot;Poll interval: {self.config.poll_interval_seconds} seconds&quot;)&#10;        self.logger.info(f&quot;Auto-sync enabled: {self.config.auto_sync}&quot;)&#10;        &#10;        # Setup signal handlers for graceful shutdown&#10;        signal.signal(signal.SIGINT, self._signal_handler)&#10;        signal.signal(signal.SIGTERM, self._signal_handler)&#10;        &#10;        # Run the monitoring loop&#10;        try:&#10;            asyncio.run(self._monitor_loop())&#10;        except KeyboardInterrupt:&#10;            self.logger.info(&quot;Received interrupt signal&quot;)&#10;        finally:&#10;            self.stop()&#10;    &#10;    def stop(self):&#10;        &quot;&quot;&quot;Stop the monitoring service&quot;&quot;&quot;&#10;        if not self.is_running:&#10;            return&#10;        &#10;        self.logger.info(&quot;Stopping EPIC Change Monitor&quot;)&#10;        self.is_running = False&#10;        self.executor.shutdown(wait=True)&#10;        self.logger.info(&quot;EPIC Change Monitor stopped&quot;)&#10;    &#10;    def _signal_handler(self, signum, frame):&#10;        &quot;&quot;&quot;Handle shutdown signals&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Received signal {signum}, shutting down gracefully...&quot;)&#10;        self.stop()&#10;        sys.exit(0)&#10;    &#10;    def get_status(self) -&gt; Dict:&#10;        &quot;&quot;&quot;Get current monitoring status&quot;&quot;&quot;&#10;        status = {&#10;            'is_running': self.is_running,&#10;            'config': asdict(self.config),&#10;            'monitored_epics': {},&#10;            'last_update': datetime.now().isoformat()&#10;        }&#10;        &#10;        for epic_id, state in self.monitored_epics.items():&#10;            status['monitored_epics'][epic_id] = {&#10;                'last_check': state.last_check.isoformat(),&#10;                'consecutive_errors': state.consecutive_errors,&#10;                'has_snapshot': state.last_snapshot is not None,&#10;                'last_sync_result': state.last_sync_result&#10;            }&#10;        &#10;        return status&#10;    &#10;    def force_check(self, epic_id: Optional[str] = None) -&gt; Dict:&#10;        &quot;&quot;&quot;Force a check for changes (optionally for specific EPIC)&quot;&quot;&quot;&#10;        results = {}&#10;        &#10;        epics_to_check = [epic_id] if epic_id else list(self.monitored_epics.keys())&#10;        &#10;        for eid in epics_to_check:&#10;            if eid in self.monitored_epics:&#10;                try:&#10;                    has_changes = self._check_epic_changes(eid)&#10;                    results[eid] = {&#10;                        'has_changes': has_changes,&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;                    &#10;                    if has_changes and self.config.auto_sync:&#10;                        sync_result = self._sync_epic(eid)&#10;                        results[eid]['sync_result'] = {&#10;                            'success': sync_result.sync_successful,&#10;                            'created_stories': sync_result.created_stories,&#10;                            'updated_stories': sync_result.updated_stories,&#10;                            'error_message': sync_result.error_message&#10;                        }&#10;                except Exception as e:&#10;                    results[eid] = {&#10;                        'error': str(e),&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;        &#10;        return results&#10;&#10;&#10;def load_config_from_file(config_file: str) -&gt; MonitorConfig:&#10;    &quot;&quot;&quot;Load monitor configuration from JSON file&quot;&quot;&quot;&#10;    try:&#10;        with open(config_file, 'r') as f:&#10;            config_data = json.load(f)&#10;        return MonitorConfig(**config_data)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to load config from {config_file}: {e}&quot;)&#10;        return MonitorConfig()&#10;&#10;&#10;def create_default_config(config_file: str = &quot;monitor_config.json&quot;):&#10;    &quot;&quot;&quot;Create a default configuration file&quot;&quot;&quot;&#10;    default_config = MonitorConfig(&#10;        poll_interval_seconds=300,  # 5 minutes&#10;        max_concurrent_syncs=3,&#10;        snapshot_directory=&quot;snapshots&quot;,&#10;        log_level=&quot;INFO&quot;,&#10;        epic_ids=[&quot;12345&quot;, &quot;67890&quot;],  # Example EPIC IDs&#10;        auto_sync=True,&#10;        retry_attempts=3,&#10;        retry_delay_seconds=60&#10;    )&#10;    &#10;    with open(config_file, 'w') as f:&#10;        json.dump(asdict(default_config), f, indent=2)&#10;    &#10;    print(f&quot;Created default configuration file: {config_file}&quot;)&#10;    return default_config&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;Background monitoring service for EPIC change detection and automatic synchronization.&#10;&quot;&quot;&quot;&#10;&#10;import asyncio&#10;import json&#10;import logging&#10;import os&#10;import signal&#10;import sys&#10;import time&#10;from datetime import datetime, timedelta&#10;from pathlib import Path&#10;from typing import Dict, List, Optional, Set, ClassVar&#10;from dataclasses import dataclass, asdict&#10;from concurrent.futures import ThreadPoolExecutor&#10;&#10;from src.agent import StoryExtractionAgent&#10;from src.models import EpicSyncResult&#10;&#10;&#10;@dataclass&#10;class MonitorConfig:&#10;    &quot;&quot;&quot;Configuration for the EPIC monitor&quot;&quot;&quot;&#10;    OPENAI_RETRY_DELAY: ClassVar[int] = int(os.getenv('OPENAI_RETRY_DELAY', 5))&#10;    poll_interval_seconds: int = 300  # 5 minutes default&#10;    max_concurrent_syncs: int = 3&#10;    snapshot_directory: str = &quot;snapshots&quot;&#10;    log_level: str = &quot;INFO&quot;&#10;    epic_ids: List[str] = None&#10;    auto_sync: bool = True&#10;    notification_webhook: Optional[str] = None&#10;    retry_attempts: int = 3&#10;    retry_delay_seconds: int = 60&#10;&#10;&#10;@dataclass&#10;class EpicMonitorState:&#10;    &quot;&quot;&quot;State tracking for a monitored EPIC&quot;&quot;&quot;&#10;    epic_id: str&#10;    last_check: datetime&#10;    last_snapshot: Optional[Dict] = None&#10;    consecutive_errors: int = 0&#10;    last_sync_result: Optional[Dict] = None&#10;&#10;&#10;class EpicChangeMonitor:&#10;    &quot;&quot;&quot;Background service that monitors EPICs for changes and triggers synchronization&quot;&quot;&quot;&#10;    &#10;    def __init__(self, config: MonitorConfig):&#10;        self.config = config&#10;        self.agent = StoryExtractionAgent()&#10;        self.logger = self._setup_logger()&#10;        self.is_running = False&#10;        self.monitored_epics: Dict[str, EpicMonitorState] = {}&#10;        self.executor = ThreadPoolExecutor(max_workers=config.max_concurrent_syncs)&#10;        self.snapshot_dir = Path(config.snapshot_directory)&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;        # ThreadPoolExecutor for async syncs&#10;        self.snapshot_dir.mkdir(exist_ok=True)&#10;&#10;        # Load existing snapshots&#10;        self._load_existing_snapshots()&#10;    &#10;    def _setup_logger(self) -&gt; logging.Logger:&#10;        &quot;&quot;&quot;Setup logging for the monitor&quot;&quot;&quot;&#10;        logger = logging.getLogger(&quot;EpicChangeMonitor&quot;)&#10;        logger.setLevel(getattr(logging, self.config.log_level.upper()))&#10;        if not logger.handlers:&#10;            # Console handler&#10;            console_handler = logging.StreamHandler()&#10;            console_formatter = logging.Formatter(&#10;                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'&#10;            )&#10;            console_handler.setFormatter(console_formatter)&#10;            logger.addHandler(console_handler)&#10;            # File handler&#10;            log_file = Path(&quot;logs&quot;) / &quot;epic_monitor.log&quot;&#10;            log_file.parent.mkdir(exist_ok=True)&#10;            file_handler = logging.FileHandler(log_file)&#10;            file_handler.setFormatter(console_formatter)&#10;            logger.addHandler(file_handler)&#10;        return logger&#10;&#10;    def _load_existing_snapshots(self):&#10;        for epic_id in self.config.epic_ids or []:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            if snapshot_file.exists():&#10;                try:&#10;                    with open(snapshot_file, 'r') as f:&#10;                        snapshot_data = json.load(f)&#10;                    &#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=snapshot_data&#10;                    )&#10;                    self.logger.info(f&quot;Loaded existing snapshot for EPIC {epic_id}&quot;)&#10;                except Exception as e:&#10;                    self.logger.error(f&quot;Failed to load snapshot for EPIC {epic_id}: {e}&quot;)&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now()&#10;                    )&#10;            else:&#10;                self.monitored_epics[epic_id] = EpicMonitorState(&#10;                    epic_id=epic_id,&#10;                    last_check=datetime.now()&#10;                )&#10;    &#10;    def add_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Add an EPIC to monitoring and trigger immediate check/sync.&quot;&quot;&quot;&#10;        try:&#10;            if epic_id not in self.monitored_epics:&#10;                # Get initial snapshot&#10;                initial_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                if initial_snapshot:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=initial_snapshot,&#10;                        consecutive_errors=0&#10;                    )&#10;                    self._save_snapshot(epic_id, initial_snapshot)&#10;                    self.logger.info(f&quot;Added EPIC {epic_id} to monitoring and will check for changes immediately.&quot;)&#10;                    # Immediately check and sync the new Epic&#10;                    if self._check_epic_changes(epic_id):&#10;                        if self.config.auto_sync:&#10;                            self.logger.info(f&quot;Immediately synchronizing new EPIC {epic_id} after detection.&quot;)&#10;                            self._sync_epic(epic_id)&#10;                    return True&#10;                else:&#10;                    self.monitored_epics[epic_id] = EpicMonitorState(&#10;                        epic_id=epic_id,&#10;                        last_check=datetime.now(),&#10;                        last_snapshot=None,&#10;                        consecutive_errors=1&#10;                    )&#10;                    self.logger.warning(f&quot;Added EPIC {epic_id} to monitoring, but could not fetch initial snapshot. Will retry.&quot;)&#10;                    return False&#10;            else:&#10;                self.logger.warning(f&quot;EPIC {epic_id} is already being monitored&quot;)&#10;                return True&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to add EPIC {epic_id} to monitoring: {e}&quot;)&#10;            return False&#10;    &#10;    def remove_epic(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Remove an EPIC from monitoring&quot;&quot;&quot;&#10;        if epic_id in self.monitored_epics:&#10;            del self.monitored_epics[epic_id]&#10;            self.logger.info(f&quot;Removed EPIC {epic_id} from monitoring&quot;)&#10;            return True&#10;        return False&#10;    &#10;    def _save_snapshot(self, epic_id: str, snapshot: Dict):&#10;        &quot;&quot;&quot;Save snapshot to file&quot;&quot;&quot;&#10;        try:&#10;            snapshot_file = self.snapshot_dir / f&quot;epic_{epic_id}.json&quot;&#10;            with open(snapshot_file, 'w') as f:&#10;                json.dump(snapshot, f, indent=2)&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to save snapshot for EPIC {epic_id}: {e}&quot;)&#10;    &#10;    def _check_epic_changes(self, epic_id: str) -&gt; bool:&#10;        &quot;&quot;&quot;Check if an EPIC has changes. Remove from monitoring if undetectable for 3 retries.&quot;&quot;&quot;&#10;        try:&#10;            epic_state = self.monitored_epics[epic_id]&#10;            current_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;            &#10;            if not current_snapshot:&#10;                epic_state.consecutive_errors += 1&#10;                self.logger.warning(f&quot;Failed to get current snapshot for EPIC {epic_id} (consecutive errors: {epic_state.consecutive_errors})&quot;)&#10;                if epic_state.consecutive_errors &gt;= 3:&#10;                    self.logger.error(f&quot;EPIC {epic_id} could not be detected after 3 retries. Removing from monitoring.&quot;)&#10;                    self.remove_epic(epic_id)&#10;                return False&#10;            &#10;            # Reset error counter on successful snapshot&#10;            epic_state.consecutive_errors = 0&#10;            &#10;            # Compare with last known snapshot&#10;            if epic_state.last_snapshot:&#10;                last_hash = epic_state.last_snapshot.get('content_hash', '')&#10;                current_hash = current_snapshot.get('content_hash', '')&#10;                &#10;                if last_hash != current_hash:&#10;                    self.logger.info(f&quot;Changes detected in EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Previous hash: {last_hash[:16]}...&quot;)&#10;                    self.logger.info(f&quot;  Current hash:  {current_hash[:16]}...&quot;)&#10;                    # Update snapshot for next check&#10;                    epic_state.last_snapshot = current_snapshot&#10;                    self._save_snapshot(epic_id, current_snapshot)&#10;                    return True&#10;                else:&#10;                    self.logger.info(f&quot;No changes detected in EPIC {epic_id}&quot;)&#10;                    return False&#10;            else:&#10;                # First check, save current snapshot&#10;                self.logger.info(f&quot;Initial snapshot saved for EPIC {epic_id}. Triggering extraction and sync.&quot;)&#10;                epic_state.last_snapshot = current_snapshot&#10;                self._save_snapshot(epic_id, current_snapshot)&#10;                return True  # Always treat as change to trigger sync for new Epics&#10;&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Error checking changes for EPIC {epic_id}: {e}&quot;)&#10;            if epic_id in self.monitored_epics:&#10;                self.monitored_epics[epic_id].consecutive_errors += 1&#10;                if self.monitored_epics[epic_id].consecutive_errors &gt;= 3:&#10;                    self.logger.error(f&quot;EPIC {epic_id} could not be detected after 3 retries. Removing from monitoring.&quot;)&#10;                    self.remove_epic(epic_id)&#10;            return False&#10;    &#10;    def _sync_epic(self, epic_id: str) -&gt; EpicSyncResult:&#10;        &quot;&quot;&quot;Synchronize an EPIC with retry logic&quot;&quot;&quot;&#10;        epic_state = self.monitored_epics[epic_id]&#10;        &#10;        for attempt in range(self.config.retry_attempts):&#10;            try:&#10;                self.logger.info(f&quot;Synchronizing EPIC {epic_id} (attempt {attempt + 1})&quot;)&#10;                &#10;                result = self.agent.synchronize_epic(&#10;                    epic_id=epic_id,&#10;                    stored_snapshot=epic_state.last_snapshot&#10;                )&#10;                &#10;                if result.sync_successful:&#10;                    # Update snapshot after successful sync&#10;                    new_snapshot = self.agent.get_epic_snapshot(epic_id)&#10;                    if new_snapshot:&#10;                        epic_state.last_snapshot = new_snapshot&#10;                        self._save_snapshot(epic_id, new_snapshot)&#10;                    &#10;                    # Store sync result&#10;                    epic_state.last_sync_result = {&#10;                        'timestamp': datetime.now().isoformat(),&#10;                        'success': True,&#10;                        'created_stories': result.created_stories,&#10;                        'updated_stories': result.updated_stories,&#10;                        'unchanged_stories': result.unchanged_stories&#10;                    }&#10;                    &#10;                    self.logger.info(f&quot;Successfully synchronized EPIC {epic_id}&quot;)&#10;                    self.logger.info(f&quot;  Created: {len(result.created_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Updated: {len(result.updated_stories)} stories&quot;)&#10;                    self.logger.info(f&quot;  Unchanged: {len(result.unchanged_stories)} stories&quot;)&#10;                    &#10;                    return result&#10;                else:&#10;                    self.logger.error(f&quot;Sync failed for EPIC {epic_id}: {result.error_message}&quot;)&#10;                    if attempt &lt; self.config.retry_attempts - 1:&#10;                        self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                        time.sleep(self.config.retry_delay_seconds)&#10;                    &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Exception during sync of EPIC {epic_id}: {e}&quot;)&#10;                if attempt &lt; self.config.retry_attempts - 1:&#10;                    self.logger.info(f&quot;Retrying in {self.config.retry_delay_seconds} seconds...&quot;)&#10;                    time.sleep(self.config.retry_delay_seconds)&#10;        &#10;        # All attempts failed&#10;        epic_state.last_sync_result = {&#10;            'timestamp': datetime.now().isoformat(),&#10;            'success': False,&#10;            'error': f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        }&#10;        &#10;        return EpicSyncResult(&#10;            epic_id=epic_id,&#10;            epic_title=&quot;&quot;,&#10;            sync_successful=False,&#10;            error_message=f&quot;Failed after {self.config.retry_attempts} attempts&quot;&#10;        )&#10;    &#10;    async def _monitor_loop(self):&#10;        &quot;&quot;&quot;Main monitoring loop&quot;&quot;&quot;&#10;        self.logger.info(&quot;Starting EPIC monitoring loop&quot;)&#10;        &#10;        while self.is_running:&#10;            try:&#10;                # Auto-detect new Epics at the start of each cycle&#10;                self.update_monitored_epics()&#10;                # Check each monitored EPIC&#10;                sync_tasks = []&#10;                &#10;                for epic_id in list(self.monitored_epics.keys()):&#10;                    try:&#10;                        epic_state = self.monitored_epics[epic_id]&#10;                        &#10;                        # Skip if too many consecutive errors&#10;                        if epic_state.consecutive_errors &gt;= 5:&#10;                            self.logger.warning(f&quot;Skipping EPIC {epic_id} due to consecutive errors&quot;)&#10;                            continue&#10;                        &#10;                        # Check for changes&#10;                        if self._check_epic_changes(epic_id):&#10;                            if self.config.auto_sync:&#10;                                # Schedule sync&#10;                                future = asyncio.get_event_loop().run_in_executor(&#10;                                    self.executor, self._sync_epic, epic_id&#10;                                )&#10;                                sync_tasks.append((epic_id, future))&#10;                            else:&#10;                                self.logger.info(f&quot;Changes detected in EPIC {epic_id}, but auto-sync is disabled&quot;)&#10;                        &#10;                        # Update last check time&#10;                        epic_state.last_check = datetime.now()&#10;                        &#10;                    except Exception as e:&#10;                        self.logger.error(f&quot;Error processing EPIC {epic_id}: {e}&quot;)&#10;                        import traceback&#10;                        self.logger.error(traceback.format_exc())&#10;&#10;                # Wait for sync tasks to complete&#10;                if sync_tasks:&#10;                    self.logger.info(f&quot;Running {len(sync_tasks)} synchronization tasks&quot;)&#10;                    for epic_id, future in sync_tasks:&#10;                        try:&#10;                            await future&#10;                        except Exception as e:&#10;                            self.logger.error(f&quot;Sync task failed for EPIC {epic_id}: {e}&quot;)&#10;                            import traceback&#10;                            self.logger.error(traceback.format_exc())&#10;&#10;                # Wait before next polling cycle&#10;                self.logger.debug(f&quot;Monitoring cycle complete, sleeping for {self.config.poll_interval_seconds} seconds&quot;)&#10;                await asyncio.sleep(self.config.poll_interval_seconds)&#10;                &#10;            except Exception as e:&#10;                self.logger.error(f&quot;Error in monitoring loop: {e}&quot;)&#10;                import traceback&#10;                self.logger.error(traceback.format_exc())&#10;                await asyncio.sleep(60)  # Wait a minute before retrying&#10;    &#10;    def fetch_all_epic_ids(self) -&gt; List[str]:&#10;        &quot;&quot;&quot;Fetch all Epic IDs from Azure DevOps (filtered by work item type 'Epic').&quot;&quot;&quot;&#10;        try:&#10;            requirements = self.agent.ado_client.get_requirements(work_item_type=&quot;Epic&quot;)&#10;            return [str(req.id) for req in requirements]&#10;        except Exception as e:&#10;            self.logger.error(f&quot;Failed to fetch all Epics: {e}&quot;)&#10;            return []&#10;&#10;    def update_monitored_epics(self):&#10;        &quot;&quot;&quot;Update the monitored Epics set by auto-detecting new Epics.&quot;&quot;&quot;&#10;        all_epic_ids = set(self.fetch_all_epic_ids())&#10;        current_epic_ids = set(self.monitored_epics.keys())&#10;        new_epics = all_epic_ids - current_epic_ids&#10;        for epic_id in new_epics:&#10;            self.logger.info(f&quot;Auto-detect: Adding new Epic {epic_id} to monitoring.&quot;)&#10;            self.add_epic(epic_id)&#10;        # Optionally, remove Epics that no longer exist in ADO&#10;        # removed_epics = current_epic_ids - all_epic_ids&#10;        # for epic_id in removed_epics:&#10;        #     self.logger.info(f&quot;Auto-detect: Removing Epic {epic_id} (no longer exists in ADO).&quot;)&#10;        #     self.monitored_epics.pop(epic_id, None)&#10;&#10;    def start(self):&#10;        &quot;&quot;&quot;Start the monitoring service&quot;&quot;&quot;&#10;        if self.is_running:&#10;            self.logger.warning(&quot;Monitor is already running&quot;)&#10;            return&#10;        &#10;        self.is_running = True&#10;        self.logger.info(&quot;Starting EPIC Change Monitor&quot;)&#10;        self.logger.info(f&quot;Monitoring {len(self.monitored_epics)} EPICs&quot;)&#10;        self.logger.info(f&quot;Poll interval: {self.config.poll_interval_seconds} seconds&quot;)&#10;        self.logger.info(f&quot;Auto-sync enabled: {self.config.auto_sync}&quot;)&#10;        &#10;        # Setup signal handlers for graceful shutdown&#10;        signal.signal(signal.SIGINT, self._signal_handler)&#10;        signal.signal(signal.SIGTERM, self._signal_handler)&#10;        &#10;        # Run the monitoring loop&#10;        try:&#10;            asyncio.run(self._monitor_loop())&#10;        except KeyboardInterrupt:&#10;            self.logger.info(&quot;Received interrupt signal&quot;)&#10;        finally:&#10;            self.stop()&#10;    &#10;    def stop(self):&#10;        &quot;&quot;&quot;Stop the monitoring service&quot;&quot;&quot;&#10;        if not self.is_running:&#10;            return&#10;        &#10;        self.logger.info(&quot;Stopping EPIC Change Monitor&quot;)&#10;        self.is_running = False&#10;        self.executor.shutdown(wait=True)&#10;        self.logger.info(&quot;EPIC Change Monitor stopped&quot;)&#10;    &#10;    def _signal_handler(self, signum, frame):&#10;        &quot;&quot;&quot;Handle shutdown signals&quot;&quot;&quot;&#10;        self.logger.info(f&quot;Received signal {signum}, shutting down gracefully...&quot;)&#10;        self.stop()&#10;        sys.exit(0)&#10;    &#10;    def get_status(self) -&gt; Dict:&#10;        &quot;&quot;&quot;Get current monitoring status&quot;&quot;&quot;&#10;        status = {&#10;            'is_running': self.is_running,&#10;            'config': asdict(self.config),&#10;            'monitored_epics': {},&#10;            'last_update': datetime.now().isoformat()&#10;        }&#10;        &#10;        for epic_id, state in self.monitored_epics.items():&#10;            status['monitored_epics'][epic_id] = {&#10;                'last_check': state.last_check.isoformat(),&#10;                'consecutive_errors': state.consecutive_errors,&#10;                'has_snapshot': state.last_snapshot is not None,&#10;                'last_sync_result': state.last_sync_result&#10;            }&#10;        &#10;        return status&#10;    &#10;    def force_check(self, epic_id: Optional[str] = None) -&gt; Dict:&#10;        &quot;&quot;&quot;Force a check for changes (optionally for specific EPIC)&quot;&quot;&quot;&#10;        results = {}&#10;        &#10;        epics_to_check = [epic_id] if epic_id else list(self.monitored_epics.keys())&#10;        &#10;        for eid in epics_to_check:&#10;            if eid in self.monitored_epics:&#10;                try:&#10;                    has_changes = self._check_epic_changes(eid)&#10;                    results[eid] = {&#10;                        'has_changes': has_changes,&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;                    &#10;                    if has_changes and self.config.auto_sync:&#10;                        sync_result = self._sync_epic(eid)&#10;                        results[eid]['sync_result'] = {&#10;                            'success': sync_result.sync_successful,&#10;                            'created_stories': sync_result.created_stories,&#10;                            'updated_stories': sync_result.updated_stories,&#10;                            'error_message': sync_result.error_message&#10;                        }&#10;                except Exception as e:&#10;                    results[eid] = {&#10;                        'error': str(e),&#10;                        'check_time': datetime.now().isoformat()&#10;                    }&#10;        &#10;        return results&#10;&#10;&#10;def load_config_from_file(config_file: str) -&gt; MonitorConfig:&#10;    &quot;&quot;&quot;Load monitor configuration from JSON file&quot;&quot;&quot;&#10;    try:&#10;        with open(config_file, 'r') as f:&#10;            config_data = json.load(f)&#10;        return MonitorConfig(**config_data)&#10;    except Exception as e:&#10;        logging.error(f&quot;Failed to load config from {config_file}: {e}&quot;)&#10;        return MonitorConfig()&#10;&#10;&#10;def create_default_config(config_file: str = &quot;monitor_config.json&quot;):&#10;    &quot;&quot;&quot;Create a default configuration file&quot;&quot;&quot;&#10;    default_config = MonitorConfig(&#10;        poll_interval_seconds=300,  # 5 minutes&#10;        max_concurrent_syncs=3,&#10;        snapshot_directory=&quot;snapshots&quot;,&#10;        log_level=&quot;INFO&quot;,&#10;        epic_ids=[&quot;12345&quot;, &quot;67890&quot;],  # Example EPIC IDs&#10;        auto_sync=True,&#10;        retry_attempts=3,&#10;        retry_delay_seconds=60&#10;    )&#10;    &#10;    with open(config_file, 'w') as f:&#10;        json.dump(asdict(default_config), f, indent=2)&#10;    &#10;    print(f&quot;Created default configuration file: {config_file}&quot;)&#10;    return default_config" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>